import cv2
import torch
import numpy as np
import time
import argparse
import threading
import gc
import psutil
import subprocess
import os
from datetime import datetime

# ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ ë¶€ëª¨ í´ë”(guideWay)ë¡œ ë³€ê²½ 
import sys 
script_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(script_dir)
os.chdir(parent_dir)
print(f"Working directory changed to: {os.getcwd()}") 

sys.path.insert(0, parent_dir) 

from efficientvit.models.efficientvit.seg import EfficientViTSeg
import torchvision.transforms as transforms

class JetsonOptimizer:
    """ì ¯ìŠ¨ í•˜ë“œì›¨ì–´ ìµœì í™” í´ë˜ìŠ¤"""
    
    @staticmethod
    def set_max_performance():
        """ì ¯ìŠ¨ì„ ìµœëŒ€ ì„±ëŠ¥ ëª¨ë“œë¡œ ì„¤ì •"""
        try:
            print("Setting Jetson to maximum performance mode...")
            
            subprocess.run(['sudo', 'nvpmodel', '-m', '0'], check=True)
            print("âœ“ Power mode set to maximum")
            
            subprocess.run(['sudo', 'jetson_clocks'], check=True)
            print("âœ“ Clocks maximized")
            
            gpu_gov_path = '/sys/devices/gpu.0/devfreq/17000000.gv11b/governor'
            if os.path.exists(gpu_gov_path):
                subprocess.run(['sudo', 'sh', '-c', f'echo performance > {gpu_gov_path}'], check=True)
                print("âœ“ GPU governor set to performance")
                
            cpu_count = psutil.cpu_count()
            for i in range(cpu_count):
                cpu_gov_path = f'/sys/devices/system/cpu/cpu{i}/cpufreq/scaling_governor'
                if os.path.exists(cpu_gov_path):
                    subprocess.run(['sudo', 'sh', '-c', f'echo performance > {cpu_gov_path}'], check=True)
            print(f"âœ“ CPU governors set to performance for {cpu_count} cores")
            
        except subprocess.CalledProcessError as e:
            print(f"Warning: Some optimization commands failed: {e}")
        except Exception as e:
            print(f"Warning: Optimization setup failed: {e}")

class EfficientViTModelManager:
    """EfficientViT ëª¨ë¸ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    AVAILABLE_MODELS = {
        'efficientvit_seg_b0': {
            'input_size': (512, 512),
            'description': 'Smallest, fastest model - good for real-time',
            'params': '0.7M'
        },
        'efficientvit_seg_b1': {
            'input_size': (512, 512), 
            'description': 'Balanced speed and accuracy',
            'params': '4.8M'
        },
        'efficientvit_seg_b2': {
            'input_size': (512, 512),
            'description': 'Higher accuracy, moderate speed',
            'params': '15.1M'
        },
        'efficientvit_seg_b3': {
            'input_size': (512, 512),
            'description': 'Best accuracy, slower inference',
            'params': '39.8M'
        },
        'efficientvit_seg_l1': {
            'input_size': (512, 512),
            'description': 'Large model - highest accuracy',
            'params': '40.5M'
        },
        'efficientvit_seg_l2': {
            'input_size': (512, 512),
            'description': 'Largest model - best quality',
            'params': '74.8M'
        }
    }
    
    @classmethod
    def list_models(cls):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¶œë ¥"""
        print("\n=== Available EfficientViT Models ===")
        for model_name, info in cls.AVAILABLE_MODELS.items():
            print(f"{model_name}:")
            print(f"  - Parameters: {info['params']}")
            print(f"  - Description: {info['description']}")
            print(f"  - Input size: {info['input_size']}")
            print()
    
    @classmethod
    def get_model_info(cls, model_name):
        """íŠ¹ì • ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return cls.AVAILABLE_MODELS.get(model_name, None)

class RealTimeCameraInference:
    def __init__(self, model_name="efficientvit_seg_l1", device="cuda", optimize_jetson=True):
        print(f"\n=== Initializing Real-time Camera Inference ===")
        
        # ëª¨ë¸ ì •ë³´ í™•ì¸
        model_info = EfficientViTModelManager.get_model_info(model_name)
        if model_info is None:
            print(f"Warning: Unknown model {model_name}")
            EfficientViTModelManager.list_models()
            raise ValueError(f"Model {model_name} not supported")
        
        self.model_name = model_name
        self.model_info = model_info
        self.device = device if torch.cuda.is_available() else "cpu"
        
        print(f"Selected Model: {model_name}")
        print(f"Model Info: {model_info['description']}")
        print(f"Parameters: {model_info['params']}")
        print(f"Device: {self.device}")
        
        # ì‹œìŠ¤í…œ ìµœì í™”
        if optimize_jetson:
            JetsonOptimizer.set_max_performance()
        
        # CUDA ìµœì í™” ì„¤ì •
        if torch.cuda.is_available():
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            print("âœ“ CUDA optimizations enabled")
        
        # ëª¨ë¸ ë¡œë“œ
        self.model = self.load_model()
        
        # í´ë˜ìŠ¤ ì„¤ì • (Cityscapes 19ê°œ í´ë˜ìŠ¤)
        self.class_names = [
            'road', 'sidewalk', 'building', 'wall', 'fence', 'pole', 'traffic_light',
            'traffic_sign', 'vegetation', 'terrain', 'sky', 'person', 'rider', 'car',
            'truck', 'bus', 'train', 'motorcycle', 'bicycle'
        ]
        self.num_classes = len(self.class_names)
        self.class_colors = self.generate_color_palette(self.num_classes)
        
        # ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
        self.input_size = model_info['input_size']
        self.transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize(self.input_size),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        # ì‹¤ì‹œê°„ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë³€ìˆ˜
        self.is_processing = threading.Lock()
        self.latest_frame = None
        self.frame_available = threading.Event()
        self.stop_processing = threading.Event()
        
        print("âœ“ Real-time inference initialization completed\n")
    
    def load_model(self):
        """EfficientViT ëª¨ë¸ ë¡œë“œ"""
        print(f"ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_name}...")
        
        try:
            from efficientvit.seg_model_zoo import create_efficientvit_seg_model, REGISTERED_EFFICIENTVIT_SEG_MODEL
            
            model_mapping = {
                'efficientvit_seg_b0': 'efficientvit-seg-b0',
                'efficientvit_seg_b1': 'efficientvit-seg-b1', 
                'efficientvit_seg_b2': 'efficientvit-seg-b2',
                'efficientvit_seg_b3': 'efficientvit-seg-b3',
                'efficientvit_seg_l1': 'efficientvit-seg-l1',
                'efficientvit_seg_l2': 'efficientvit-seg-l2'
            }
            
            model_name_mapped = model_mapping.get(self.model_name, 'efficientvit-seg-b0')
            
            # ë™ì  ê²½ë¡œ í•´ê²°
            if model_name_mapped in REGISTERED_EFFICIENTVIT_SEG_MODEL:
                model_builder, norm_eps, registered_path = REGISTERED_EFFICIENTVIT_SEG_MODEL[model_name_mapped]
                
                current_dir = os.getcwd()
                efficientvit_path = os.path.join(current_dir, "efficientvit", registered_path)
                
                print(f"ë“±ë¡ëœ ê²½ë¡œ: {registered_path}")
                print(f"ì‹¤ì œ í™•ì¸ ê²½ë¡œ: {efficientvit_path}")
                
                if os.path.exists(efficientvit_path):
                    print(f"âœ“ ë¡œì»¬ ì²´í¬í¬ì¸íŠ¸ ë°œê²¬: {efficientvit_path}")
                    model = create_efficientvit_seg_model(
                        name=model_name_mapped,
                        dataset="cityscapes",
                        weight_url=efficientvit_path,
                        n_classes=19
                    )
                    print(f"âœ“ ë¡œì»¬ì—ì„œ ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_name_mapped}")
                else:
                    print(f"ë¡œì»¬ íŒŒì¼ ì—†ìŒ, ì˜¨ë¼ì¸ ë‹¤ìš´ë¡œë“œ ì‹œë„...")
                    model = create_efficientvit_seg_model(
                        name=model_name_mapped,
                        dataset="cityscapes",
                        pretrained=True,
                        weight_url=None,
                        n_classes=19
                    )
                    print(f"âœ“ ì˜¨ë¼ì¸ì—ì„œ ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_name_mapped}")
            else:
                print(f"ë“±ë¡ ì •ë³´ ì—†ìŒ, ì˜¨ë¼ì¸ ë‹¤ìš´ë¡œë“œ...")
                model = create_efficientvit_seg_model(
                    name=model_name_mapped,
                    dataset="cityscapes",
                    pretrained=True,
                    n_classes=19
                )
                print(f"âœ“ ì˜¨ë¼ì¸ì—ì„œ ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_name_mapped}")
                
        except Exception as e:
            print(f"EfficientViT ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
        
        # ëª¨ë¸ ìµœì í™”
        model = model.to(self.device)
        model.eval()
        
        # JIT ì»´íŒŒì¼
        if self.device == "cuda" and hasattr(torch, 'jit'):
            try:
                dummy_input = torch.randn(1, 3, *self.model_info['input_size']).to(self.device)
                with torch.no_grad():
                    _ = model(dummy_input)
                model = torch.jit.trace(model, dummy_input)
                print("âœ“ TorchScriptë¡œ ëª¨ë¸ ìµœì í™” ì™„ë£Œ")
            except Exception as e:
                print(f"TorchScript ìµœì í™” ì‹¤íŒ¨: {e}")
        
        return model
    
    def generate_color_palette(self, num_classes):
        """í´ë˜ìŠ¤ ìˆ˜ì— ë§ëŠ” ìƒ‰ìƒ íŒ”ë ˆíŠ¸ ìƒì„±"""
        predefined_colors = [
            [128, 64, 128], [244, 35, 232], [70, 70, 70], [102, 102, 156],
            [190, 153, 153], [153, 153, 153], [250, 170, 30], [220, 220, 0],
            [107, 142, 35], [152, 251, 152], [70, 130, 180], [220, 20, 60],
            [255, 0, 0], [0, 0, 142], [0, 0, 70], [0, 60, 100], [0, 80, 100],
            [0, 0, 230], [119, 11, 32]
        ]
        
        colors = []
        for i in range(min(num_classes, len(predefined_colors))):
            colors.append(predefined_colors[i])
        
        return np.array(colors, dtype=np.uint8)
    
    def setup_camera(self, width=1280, height=720, fps=60):
        """ì¹´ë©”ë¼ ì„¤ì • - ê²€ì¦ëœ íŒŒì´í”„ë¼ì¸ ì‚¬ìš©"""
        print(f"Setting up camera: {width}x{height} @ {fps}FPS")
        
        # ê²€ì¦ëœ GStreamer íŒŒì´í”„ë¼ì¸
        gst_pipeline = (
            f"nvarguscamerasrc ! "
            f"video/x-raw(memory:NVMM), width={width}, height={height}, framerate={fps}/1 ! "
            f"nvvidconv ! video/x-raw, format=BGRx ! "
            f"videoconvert ! video/x-raw, format=BGR ! "
            f"appsink drop=true max-buffers=1"
        )
        
        cap = cv2.VideoCapture(gst_pipeline, cv2.CAP_GSTREAMER)
        
        if not cap.isOpened():
            print("âœ— Camera setup failed")
            return None
        
        # í”„ë ˆì„ ìº¡ì²˜ í…ŒìŠ¤íŠ¸
        ret, frame = cap.read()
        if not ret or frame is None:
            print("âœ— Failed to capture test frame")
            cap.release()
            return None
        
        print(f"âœ“ Camera setup completed: {frame.shape}")
        return cap
    
    def preprocess_frame(self, frame):
        """í”„ë ˆì„ ì „ì²˜ë¦¬"""
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        input_tensor = self.transform(frame_rgb).unsqueeze(0)
        return input_tensor.to(self.device, non_blocking=True)
    
    def postprocess_output(self, output, original_shape):
        """ëª¨ë¸ ì¶œë ¥ í›„ì²˜ë¦¬"""
        with torch.no_grad():
            if isinstance(output, dict):
                for key in ['out', 'seg', 'logits']:
                    if key in output:
                        logits = output[key]
                        break
                else:
                    logits = list(output.values())[0]
            else:
                logits = output
            
            # ì›ë³¸ í¬ê¸°ë¡œ ì—…ìƒ˜í”Œë§
            upsampled_logits = torch.nn.functional.interpolate(
                logits, 
                size=original_shape[:2], 
                mode='bilinear',
                align_corners=False
            )
            
            pred = torch.argmax(upsampled_logits, dim=1).squeeze()
            return pred.cpu().numpy().astype(np.uint8)
    
    def create_mask_visualization(self, segmentation_mask):
        """ë§ˆìŠ¤í¬ ì‹œê°í™” ìƒì„±"""
        colored_mask = self.class_colors[segmentation_mask % len(self.class_colors)]
        colored_mask = colored_mask[..., ::-1]  # RGB -> BGR
        
        # OpenCV í˜¸í™˜ì„ ìœ„í•´ ì—°ì†ì ì¸ ë©”ëª¨ë¦¬ ë ˆì´ì•„ì›ƒìœ¼ë¡œ ë³€í™˜
        mask_output = np.ascontiguousarray(colored_mask, dtype=np.uint8)
        
        return mask_output
    
    def camera_capture_thread(self, cap):
        """ì¹´ë©”ë¼ ìº¡ì²˜ ìŠ¤ë ˆë“œ"""
        print("Starting camera capture thread...")
        
        while not self.stop_processing.is_set():
            ret, frame = cap.read()
            if not ret:
                print("Warning: Failed to capture frame")
                continue
            
            # ì¶”ë¡  ì¤‘ì´ ì•„ë‹ ë•Œë§Œ í”„ë ˆì„ ì—…ë°ì´íŠ¸
            if not self.is_processing.locked():
                self.latest_frame = frame.copy()
                self.frame_available.set()
            
            time.sleep(0.001)
    
    def start_realtime_inference(self, duration_seconds=60, output_path="camera_output.mp4", 
                            width=1280, height=720, fps=30, show_stats=True):
        """ì‹¤ì‹œê°„ ì¶”ë¡  ì†ë„ë¥¼ ì¸¡ì •í•˜ë©´ì„œ ë™ì ìœ¼ë¡œ FPS ì¡°ì •í•˜ì—¬ ì •í™•í•œ ì‹œê°„ ë³´ì¥"""
        
        print(f"\n=== Starting Adaptive Real-time Inference ===")
        print(f"Duration: {duration_seconds} seconds")
        print(f"Output: {output_path}")
        print(f"Resolution: {width}x{height}")
        print(f"Target FPS: {fps} (will adapt to actual inference speed)")
        
        # ì¹´ë©”ë¼ ì„¤ì •
        cap = self.setup_camera(width, height, fps=60)
        if cap is None:
            print("âœ— Camera setup failed - exiting")
            return False
        
        # ì„ì‹œë¡œ ë†’ì€ FPSë¡œ VideoWriter ìƒì„± (ë‚˜ì¤‘ì— ì¬ì¸ì½”ë”©)
        temp_output = f"temp_{output_path}"
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        temp_writer = cv2.VideoWriter(temp_output, fourcc, 60, (width, height))
        
        # ìŠ¤ë ˆë“œ ì‹œì‘
        capture_thread = threading.Thread(target=self.camera_capture_thread, args=(cap,))
        inference_thread = threading.Thread(
            target=self.adaptive_realtime_thread, 
            args=(temp_writer, show_stats, duration_seconds)
        )
        
        capture_thread.start()
        inference_thread.start()
        
        try:
            print(f"Recording for {duration_seconds} seconds with adaptive FPS...")
            time.sleep(duration_seconds)
            
        except KeyboardInterrupt:
            print("\nStopping by user request...")
        
        finally:
            print("Cleaning up...")
            self.stop_processing.set()
            
            capture_thread.join(timeout=2.0)
            inference_thread.join(timeout=2.0)
            
            cap.release()
            temp_writer.release()
            
            # ì¸¡ì •ëœ ì‹¤ì œ FPSë¡œ ìµœì¢… ë¹„ë””ì˜¤ ìƒì„±
            self.create_final_video(temp_output, output_path, duration_seconds, width, height)
            
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            if os.path.exists(temp_output):
                os.remove(temp_output)
            
            print(f"âœ“ Video saved to: {output_path}")
            return True

    def adaptive_realtime_thread(self, temp_writer, show_stats, total_duration):
        """ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶”ë¡  ì†ë„ë¥¼ ì¸¡ì •í•˜ë©´ì„œ ë™ì  FPS ì¡°ì •"""
        print("Starting adaptive real-time thread...")
        
        start_time = time.time()
        inference_times = []
        inference_results = []  # (timestamp, frame) ì €ì¥
        last_result = None
        
        # FPS ì¸¡ì • ë³€ìˆ˜
        fps_update_interval = 1.0  # 1ì´ˆë§ˆë‹¤ FPS ì—…ë°ì´íŠ¸
        last_fps_update = start_time
        current_measured_fps = 1.0
        
        with torch.inference_mode():
            while not self.stop_processing.is_set():
                current_time = time.time()
                elapsed_time = current_time - start_time
                
                # ìƒˆë¡œìš´ ì¶”ë¡  ì‹¤í–‰
                if self.frame_available.wait(timeout=0.01):
                    with self.is_processing:
                        if self.latest_frame is not None:
                            frame = self.latest_frame.copy()
                            self.frame_available.clear()
                            
                            # ì¶”ë¡  ì‹¤í–‰
                            inf_start = time.time()
                            input_tensor = self.preprocess_frame(frame)
                            output = self.model(input_tensor)
                            mask = self.postprocess_output(output, frame.shape[:2])
                            inf_time = time.time() - inf_start
                            
                            inference_times.append(inf_time)
                            
                            # ì‹œê°í™”
                            result = self.create_mask_visualization(mask)
                            
                            # í†µê³„ í‘œì‹œ
                            if show_stats:
                                current_inf_fps = 1 / inf_time if inf_time > 0 else 0
                                avg_inf_fps = len(inference_times) / sum(inference_times) if inference_times else 0
                                
                                cv2.putText(result, f"Current: {current_inf_fps:.1f} | Avg: {avg_inf_fps:.1f} FPS", 
                                        (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                                cv2.putText(result, f"Measured FPS: {current_measured_fps:.1f} | Time: {elapsed_time:.1f}s", 
                                        (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                                cv2.putText(result, f"Model: {self.model_name}", 
                                        (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                            
                            last_result = result
                            # íƒ€ì„ìŠ¤íƒ¬í”„ì™€ í•¨ê»˜ ê²°ê³¼ ì €ì¥
                            inference_results.append((elapsed_time, result.copy()))
                
                # ì£¼ê¸°ì ìœ¼ë¡œ ì¸¡ì •ëœ FPS ì—…ë°ì´íŠ¸
                if current_time - last_fps_update >= fps_update_interval and inference_times:
                    current_measured_fps = len(inference_times) / sum(inference_times)
                    last_fps_update = current_time
                    print(f"Updated measured FPS: {current_measured_fps:.2f} (total inferences: {len(inference_times)})")
                
                # ì„ì‹œ íŒŒì¼ì— í˜„ì¬ ê²°ê³¼ ì €ì¥ (ë†’ì€ ì£¼ê¸°ë¡œ)
                if last_result is not None:
                    temp_writer.write(last_result)
                
                time.sleep(0.001)  # CPU ë¶€í•˜ ê°ì†Œ
        
        # ìµœì¢… ì¸¡ì •ëœ FPS ì €ì¥
        if inference_times:
            self.final_measured_fps = len(inference_times) / sum(inference_times)
            self.inference_results = inference_results
            print(f"Final measured FPS: {self.final_measured_fps:.2f}")
            print(f"Total inferences: {len(inference_times)}")
            print(f"Total inference results: {len(inference_results)}")
        else:
            self.final_measured_fps = 1.0
            self.inference_results = []

    def create_final_video(self, temp_path, final_path, duration, width, height):
        """ì¸¡ì •ëœ ì‹¤ì œ FPSë¡œ ìµœì¢… ë¹„ë””ì˜¤ ìƒì„±"""
        print("Creating final video with measured FPS...")
        
        if not hasattr(self, 'final_measured_fps') or not hasattr(self, 'inference_results'):
            print("Warning: No FPS measurement data, using original file")
            if os.path.exists(temp_path):
                os.rename(temp_path, final_path)
            return
        
        measured_fps = self.final_measured_fps
        results = self.inference_results
        
        print(f"Measured FPS: {measured_fps:.2f}")
        print(f"Creating {duration}s video at {measured_fps:.2f} FPS")
        
        # ìµœì¢… ë¹„ë””ì˜¤ ìƒì„±
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        final_writer = cv2.VideoWriter(final_path, fourcc, measured_fps, (width, height))
        
        if not results:
            print("Warning: No inference results, creating from temp file")
            # ì„ì‹œ íŒŒì¼ì—ì„œ ë³µì‚¬
            cap = cv2.VideoCapture(temp_path)
            frame_count = 0
            target_frames = int(duration * measured_fps)
            
            while frame_count < target_frames:
                ret, frame = cap.read()
                if not ret:
                    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # ë£¨í”„
                    continue
                final_writer.write(frame)
                frame_count += 1
            
            cap.release()
        else:
            # ì¶”ë¡  ê²°ê³¼ë¥¼ ì‹œê°„ì— ë§ì¶° ë°°ì¹˜
            total_frames_needed = int(duration * measured_fps)
            frame_interval = duration / total_frames_needed
            
            print(f"Target frames: {total_frames_needed}")
            print(f"Frame interval: {frame_interval:.3f}s")
            
            current_result_idx = 0
            
            for frame_idx in range(total_frames_needed):
                target_time = frame_idx * frame_interval
                
                # ê°€ì¥ ê°€ê¹Œìš´ ì¶”ë¡  ê²°ê³¼ ì°¾ê¸°
                best_result = None
                min_time_diff = float('inf')
                
                for i, (timestamp, result) in enumerate(results):
                    time_diff = abs(timestamp - target_time)
                    if time_diff < min_time_diff:
                        min_time_diff = time_diff
                        best_result = result
                        current_result_idx = i
                
                if best_result is not None:
                    final_writer.write(best_result)
                else:
                    # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ê²€ì€ í™”ë©´
                    black_frame = np.zeros((height, width, 3), dtype=np.uint8)
                    cv2.putText(black_frame, f"No inference at {target_time:.1f}s", 
                            (width//2-150, height//2), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
                    final_writer.write(black_frame)
                
                # ì§„í–‰ë¥  í‘œì‹œ
                if frame_idx % (total_frames_needed // 10) == 0:
                    progress = (frame_idx / total_frames_needed) * 100
                    print(f"Final video progress: {progress:.1f}%")
        
        final_writer.release()
        print(f"âœ“ Final video created: {final_path}")
        print(f"âœ“ Duration: {duration}s at {measured_fps:.2f} FPS")


# ğŸ”¥ main í•¨ìˆ˜ë¥¼ í´ë˜ìŠ¤ ë°–ìœ¼ë¡œ ì´ë™
def main():
    parser = argparse.ArgumentParser(description="Real-time EfficientViT Camera Inference with Adaptive FPS")
    parser.add_argument("--model", "-m", default="efficientvit_seg_l1", 
                       choices=list(EfficientViTModelManager.AVAILABLE_MODELS.keys()),
                       help="EfficientViT model name")
    parser.add_argument("--output", "-o", default=None, help="Output video path")
    parser.add_argument("--duration", "-d", type=int, default=60, help="Recording duration in seconds")
    parser.add_argument("--width", "-w", type=int, default=1280, help="Camera width")
    parser.add_argument("--height", type=int, default=720, help="Camera height")
    parser.add_argument("--fps", type=int, default=30, help="Target FPS (will adapt to actual speed)")
    parser.add_argument("--device", default="cuda", help="Device (cuda/cpu)")
    parser.add_argument("--no-optimize", action="store_true", help="Skip Jetson optimization")
    parser.add_argument("--no-stats", action="store_true", help="Hide performance overlay")
    parser.add_argument("--list-models", action="store_true", help="List available models")
    
    args = parser.parse_args()
    
    # ëª¨ë¸ ëª©ë¡ ì¶œë ¥
    if args.list_models:
        EfficientViTModelManager.list_models()
        return
    
    # ì¶œë ¥ íŒŒì¼ëª… ìë™ ìƒì„±
    if args.output is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        args.output = f"adaptive_{args.model}_{timestamp}.mp4"
    
    print(f"ğŸ¯ Adaptive FPS Mode:")
    print(f"â†’ Will measure actual inference speed in real-time")
    print(f"â†’ Final video will use measured FPS for accurate {args.duration}s duration")
    
    try:
        # ì¶”ë¡  ê°ì²´ ìƒì„±
        inferencer = RealTimeCameraInference(
            model_name=args.model,
            device=args.device,
            optimize_jetson=not args.no_optimize
        )
        
        # ì‹¤í–‰
        success = inferencer.start_realtime_inference(
            duration_seconds=args.duration,
            output_path=args.output,
            width=args.width,
            height=args.height,
            fps=args.fps,
            show_stats=not args.no_stats
        )
        
        if success:
            print(f"\nâœ… Recording completed successfully!")
            print(f"âœ… Output file: {args.output}")
            print(f"âœ… Guaranteed duration: {args.duration} seconds")
        else:
            print("âŒ Recording failed")
            
    except Exception as e:
        print(f"Error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()