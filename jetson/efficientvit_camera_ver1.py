import cv2
import torch
import numpy as np
import time
import argparse
import threading
import gc
import psutil
import subprocess
import os
from datetime import datetime

# ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ ë¶€ëª¨ í´ë”(guideWay)ë¡œ ë³€ê²½ 
import sys 
script_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(script_dir)
os.chdir(parent_dir)
print(f"Working directory changed to: {os.getcwd()}") 

sys.path.insert(0, parent_dir) 

from efficientvit.models.efficientvit.seg import EfficientViTSeg
import torchvision.transforms as transforms

class JetsonOptimizer:
    """ì ¯ìŠ¨ í•˜ë“œì›¨ì–´ ìµœì í™” í´ë˜ìŠ¤"""
    
    @staticmethod
    def set_max_performance():
        """ì ¯ìŠ¨ì„ ìµœëŒ€ ì„±ëŠ¥ ëª¨ë“œë¡œ ì„¤ì •"""
        try:
            print("Setting Jetson to maximum performance mode...")
            
            subprocess.run(['sudo', 'nvpmodel', '-m', '0'], check=True)
            print("âœ“ Power mode set to maximum")
            
            subprocess.run(['sudo', 'jetson_clocks'], check=True)
            print("âœ“ Clocks maximized")
            
            gpu_gov_path = '/sys/devices/gpu.0/devfreq/17000000.gv11b/governor'
            if os.path.exists(gpu_gov_path):
                subprocess.run(['sudo', 'sh', '-c', f'echo performance > {gpu_gov_path}'], check=True)
                print("âœ“ GPU governor set to performance")
                
            cpu_count = psutil.cpu_count()
            for i in range(cpu_count):
                cpu_gov_path = f'/sys/devices/system/cpu/cpu{i}/cpufreq/scaling_governor'
                if os.path.exists(cpu_gov_path):
                    subprocess.run(['sudo', 'sh', '-c', f'echo performance > {cpu_gov_path}'], check=True)
            print(f"âœ“ CPU governors set to performance for {cpu_count} cores")
            
        except subprocess.CalledProcessError as e:
            print(f"Warning: Some optimization commands failed: {e}")
        except Exception as e:
            print(f"Warning: Optimization setup failed: {e}")

class EfficientViTModelManager:
    """EfficientViT ëª¨ë¸ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    AVAILABLE_MODELS = {
        'efficientvit_seg_b0': {
            'input_size': (512, 512),
            'description': 'Smallest, fastest model - good for real-time',
            'params': '0.7M'
        },
        'efficientvit_seg_b1': {
            'input_size': (512, 512), 
            'description': 'Balanced speed and accuracy',
            'params': '4.8M'
        },
        'efficientvit_seg_b2': {
            'input_size': (512, 512),
            'description': 'Higher accuracy, moderate speed',
            'params': '15.1M'
        },
        'efficientvit_seg_b3': {
            'input_size': (512, 512),
            'description': 'Best accuracy, slower inference',
            'params': '39.8M'
        },
        'efficientvit_seg_l1': {
            'input_size': (512, 512),
            'description': 'Large model - highest accuracy',
            'params': '40.5M'
        },
        'efficientvit_seg_l2': {
            'input_size': (512, 512),
            'description': 'Largest model - best quality',
            'params': '74.8M'
        }
    }
    
    @classmethod
    def list_models(cls):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¶œë ¥"""
        print("\n=== Available EfficientViT Models ===")
        for model_name, info in cls.AVAILABLE_MODELS.items():
            print(f"{model_name}:")
            print(f"  - Parameters: {info['params']}")
            print(f"  - Description: {info['description']}")
            print(f"  - Input size: {info['input_size']}")
            print()
    
    @classmethod
    def get_model_info(cls, model_name):
        """íŠ¹ì • ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return cls.AVAILABLE_MODELS.get(model_name, None)

class RealTimeCameraInference:
    def __init__(self, model_name="efficientvit_seg_l1", device="cuda", optimize_jetson=True):
        print(f"\n=== Initializing Real-time Camera Inference ===")
        
        # ëª¨ë¸ ì •ë³´ í™•ì¸
        model_info = EfficientViTModelManager.get_model_info(model_name)
        if model_info is None:
            print(f"Warning: Unknown model {model_name}")
            EfficientViTModelManager.list_models()
            raise ValueError(f"Model {model_name} not supported")
        
        self.model_name = model_name
        self.model_info = model_info
        self.device = device if torch.cuda.is_available() else "cpu"
        
        print(f"Selected Model: {model_name}")
        print(f"Model Info: {model_info['description']}")
        print(f"Parameters: {model_info['params']}")
        print(f"Device: {self.device}")
        
        # ì‹œìŠ¤í…œ ìµœì í™”
        if optimize_jetson:
            JetsonOptimizer.set_max_performance()
        
        # CUDA ìµœì í™” ì„¤ì •
        if torch.cuda.is_available():
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            print("âœ“ CUDA optimizations enabled")
        
        # ëª¨ë¸ ë¡œë“œ
        self.model = self.load_model()
        
        # í´ë˜ìŠ¤ ì„¤ì • (Cityscapes 19ê°œ í´ë˜ìŠ¤)
        self.class_names = [
            'road', 'sidewalk', 'building', 'wall', 'fence', 'pole', 'traffic_light',
            'traffic_sign', 'vegetation', 'terrain', 'sky', 'person', 'rider', 'car',
            'truck', 'bus', 'train', 'motorcycle', 'bicycle'
        ]
        self.num_classes = len(self.class_names)
        self.class_colors = self.generate_color_palette(self.num_classes)
        
        # ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
        self.input_size = model_info['input_size']
        self.transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize(self.input_size),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        # ì‹¤ì‹œê°„ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë³€ìˆ˜
        self.is_processing = threading.Lock()
        self.latest_frame = None
        self.frame_available = threading.Event()
        self.stop_processing = threading.Event()
        
        print("âœ“ Real-time inference initialization completed\n")
    
    def load_model(self):
        """EfficientViT ëª¨ë¸ ë¡œë“œ"""
        print(f"ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_name}...")
        
        try:
            from efficientvit.seg_model_zoo import create_efficientvit_seg_model, REGISTERED_EFFICIENTVIT_SEG_MODEL
            
            model_mapping = {
                'efficientvit_seg_b0': 'efficientvit-seg-b0',
                'efficientvit_seg_b1': 'efficientvit-seg-b1', 
                'efficientvit_seg_b2': 'efficientvit-seg-b2',
                'efficientvit_seg_b3': 'efficientvit-seg-b3',
                'efficientvit_seg_l1': 'efficientvit-seg-l1',
                'efficientvit_seg_l2': 'efficientvit-seg-l2'
            }
            
            model_name_mapped = model_mapping.get(self.model_name, 'efficientvit-seg-b0')
            
            # ë™ì  ê²½ë¡œ í•´ê²°
            if model_name_mapped in REGISTERED_EFFICIENTVIT_SEG_MODEL:
                model_builder, norm_eps, registered_path = REGISTERED_EFFICIENTVIT_SEG_MODEL[model_name_mapped]
                
                current_dir = os.getcwd()
                efficientvit_path = os.path.join(current_dir, "efficientvit", registered_path)
                
                print(f"ë“±ë¡ëœ ê²½ë¡œ: {registered_path}")
                print(f"ì‹¤ì œ í™•ì¸ ê²½ë¡œ: {efficientvit_path}")
                
                if os.path.exists(efficientvit_path):
                    print(f"âœ“ ë¡œì»¬ ì²´í¬í¬ì¸íŠ¸ ë°œê²¬: {efficientvit_path}")
                    model = create_efficientvit_seg_model(
                        name=model_name_mapped,
                        dataset="cityscapes",
                        weight_url=efficientvit_path,
                        n_classes=19
                    )
                    print(f"âœ“ ë¡œì»¬ì—ì„œ ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_name_mapped}")
                else:
                    print(f"ë¡œì»¬ íŒŒì¼ ì—†ìŒ, ì˜¨ë¼ì¸ ë‹¤ìš´ë¡œë“œ ì‹œë„...")
                    model = create_efficientvit_seg_model(
                        name=model_name_mapped,
                        dataset="cityscapes",
                        pretrained=True,
                        weight_url=None,
                        n_classes=19
                    )
                    print(f"âœ“ ì˜¨ë¼ì¸ì—ì„œ ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_name_mapped}")
            else:
                print(f"ë“±ë¡ ì •ë³´ ì—†ìŒ, ì˜¨ë¼ì¸ ë‹¤ìš´ë¡œë“œ...")
                model = create_efficientvit_seg_model(
                    name=model_name_mapped,
                    dataset="cityscapes",
                    pretrained=True,
                    n_classes=19
                )
                print(f"âœ“ ì˜¨ë¼ì¸ì—ì„œ ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_name_mapped}")
                
        except Exception as e:
            print(f"EfficientViT ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
        
        # ëª¨ë¸ ìµœì í™”
        model = model.to(self.device)
        model.eval()
        
        # JIT ì»´íŒŒì¼
        if self.device == "cuda" and hasattr(torch, 'jit'):
            try:
                dummy_input = torch.randn(1, 3, *self.model_info['input_size']).to(self.device)
                with torch.no_grad():
                    _ = model(dummy_input)
                model = torch.jit.trace(model, dummy_input)
                print("âœ“ TorchScriptë¡œ ëª¨ë¸ ìµœì í™” ì™„ë£Œ")
            except Exception as e:
                print(f"TorchScript ìµœì í™” ì‹¤íŒ¨: {e}")
        
        return model
    
    def generate_color_palette(self, num_classes):
        """í´ë˜ìŠ¤ ìˆ˜ì— ë§ëŠ” ìƒ‰ìƒ íŒ”ë ˆíŠ¸ ìƒì„±"""
        predefined_colors = [
            [128, 64, 128], [244, 35, 232], [70, 70, 70], [102, 102, 156],
            [190, 153, 153], [153, 153, 153], [250, 170, 30], [220, 220, 0],
            [107, 142, 35], [152, 251, 152], [70, 130, 180], [220, 20, 60],
            [255, 0, 0], [0, 0, 142], [0, 0, 70], [0, 60, 100], [0, 80, 100],
            [0, 0, 230], [119, 11, 32]
        ]
        
        colors = []
        for i in range(min(num_classes, len(predefined_colors))):
            colors.append(predefined_colors[i])
        
        return np.array(colors, dtype=np.uint8)
    
    def setup_camera(self, width=1280, height=720, fps=60):
        """ì¹´ë©”ë¼ ì„¤ì • - ê²€ì¦ëœ íŒŒì´í”„ë¼ì¸ ì‚¬ìš©"""
        print(f"Setting up camera: {width}x{height} @ {fps}FPS")
        
        # ê²€ì¦ëœ GStreamer íŒŒì´í”„ë¼ì¸
        gst_pipeline = (
            f"nvarguscamerasrc ! "
            f"video/x-raw(memory:NVMM), width={width}, height={height}, framerate={fps}/1 ! "
            f"nvvidconv ! video/x-raw, format=BGRx ! "
            f"videoconvert ! video/x-raw, format=BGR ! "
            f"appsink drop=true max-buffers=1"
        )
        
        cap = cv2.VideoCapture(gst_pipeline, cv2.CAP_GSTREAMER)
        
        if not cap.isOpened():
            print("âœ— Camera setup failed")
            return None
        
        # í”„ë ˆì„ ìº¡ì²˜ í…ŒìŠ¤íŠ¸
        ret, frame = cap.read()
        if not ret or frame is None:
            print("âœ— Failed to capture test frame")
            cap.release()
            return None
        
        print(f"âœ“ Camera setup completed: {frame.shape}")
        return cap
    
    def preprocess_frame(self, frame):
        """í”„ë ˆì„ ì „ì²˜ë¦¬"""
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        input_tensor = self.transform(frame_rgb).unsqueeze(0)
        return input_tensor.to(self.device, non_blocking=True)
    
    def postprocess_output(self, output, original_shape):
        """ëª¨ë¸ ì¶œë ¥ í›„ì²˜ë¦¬"""
        with torch.no_grad():
            if isinstance(output, dict):
                for key in ['out', 'seg', 'logits']:
                    if key in output:
                        logits = output[key]
                        break
                else:
                    logits = list(output.values())[0]
            else:
                logits = output
            
            # ì›ë³¸ í¬ê¸°ë¡œ ì—…ìƒ˜í”Œë§
            upsampled_logits = torch.nn.functional.interpolate(
                logits, 
                size=original_shape[:2], 
                mode='bilinear',
                align_corners=False
            )
            
            pred = torch.argmax(upsampled_logits, dim=1).squeeze()
            return pred.cpu().numpy().astype(np.uint8)
    
    def create_mask_visualization(self, segmentation_mask):
        """ë§ˆìŠ¤í¬ ì‹œê°í™” ìƒì„±"""
        colored_mask = self.class_colors[segmentation_mask % len(self.class_colors)]
        colored_mask = colored_mask[..., ::-1]  # RGB -> BGR
        
        # return colored_mask
        # OpenCV í˜¸í™˜ì„ ìœ„í•´ ì—°ì†ì ì¸ ë©”ëª¨ë¦¬ ë ˆì´ì•„ì›ƒìœ¼ë¡œ ë³€í™˜
        return np.ascontiguousarray(colored_mask, dtype=np.uint8)
    
    def camera_capture_thread(self, cap):
        """ì¹´ë©”ë¼ ìº¡ì²˜ ìŠ¤ë ˆë“œ"""
        print("Starting camera capture thread...")
        
        while not self.stop_processing.is_set():
            ret, frame = cap.read()
            if not ret:
                print("Warning: Failed to capture frame")
                continue
            
            # ì¶”ë¡  ì¤‘ì´ ì•„ë‹ ë•Œë§Œ í”„ë ˆì„ ì—…ë°ì´íŠ¸
            if not self.is_processing.locked():
                self.latest_frame = frame.copy()
                self.frame_available.set()
            
            time.sleep(0.001)
    
    def inference_thread(self, output_writer, show_stats=True, target_fps=30):
        """ì¶”ë¡  ì²˜ë¦¬ ìŠ¤ë ˆë“œ - ì‹¤ì‹œê°„ ì†ë„ ë§ì¶¤"""
        print("Starting inference thread...")
        
        inference_times = []
        frame_count = 0
        start_real_time = time.time()
        
        with torch.inference_mode():
            while not self.stop_processing.is_set():
                # ìƒˆ í”„ë ˆì„ ëŒ€ê¸°
                if not self.frame_available.wait(timeout=1.0):
                    continue
                
                # ì¶”ë¡  ì¤‘ í”Œë˜ê·¸ ì„¤ì •
                with self.is_processing:
                    if self.latest_frame is None:
                        continue
                    
                    current_frame = self.latest_frame.copy()
                    self.frame_available.clear()
                    
                    # ì¶”ë¡  ì‹¤í–‰
                    start_time = time.time()
                    input_tensor = self.preprocess_frame(current_frame)
                    output = self.model(input_tensor)
                    segmentation_mask = self.postprocess_output(output, current_frame.shape[:2])
                    inference_time = time.time() - start_time
                    inference_times.append(inference_time)
                    
                    # ë§ˆìŠ¤í¬ ì‹œê°í™”
                    mask_output = self.create_mask_visualization(segmentation_mask)
                    
                    # ì„±ëŠ¥ ì •ë³´ í‘œì‹œ
                    if show_stats:
                        current_fps = 1 / inference_time if inference_time > 0 else 0
                        avg_fps = len(inference_times) / sum(inference_times) if inference_times else 0
                        
                        fps_text = f"Current FPS: {current_fps:.1f} | Avg FPS: {avg_fps:.1f}"
                        model_text = f"Model: {self.model_name}"
                        
                        cv2.putText(mask_output, fps_text, (10, 30), 
                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                        cv2.putText(mask_output, model_text, (10, 60), 
                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                    
                    # ğŸ”¥ ì‹¤ì‹œê°„ ì†ë„ì— ë§ì¶° í”„ë ˆì„ ë°˜ë³µ ì €ì¥
                    current_real_time = time.time() - start_real_time
                    expected_frames = int(current_real_time * target_fps)
                    
                    # í˜„ì¬ê¹Œì§€ ì €ì¥í•´ì•¼ í•  í”„ë ˆì„ ìˆ˜ë§Œí¼ ë°˜ë³µ ì €ì¥
                    while frame_count < expected_frames:
                        # ë¹„ë””ì˜¤ ì €ì¥
                        output_writer.write(mask_output)
                        frame_count += 1
                    
                    # ì£¼ê¸°ì ìœ¼ë¡œ í†µê³„ ì¶œë ¥
                    if frame_count % 100 == 0:
                        avg_fps = len(inference_times) / sum(inference_times) if inference_times else 0
                        print(f"Processed {len(inference_times)} inferences, Saved {frame_count} frames, Avg FPS: {avg_fps:.2f}")
                        
                        # ë©”ëª¨ë¦¬ ê´€ë¦¬
                        torch.cuda.empty_cache() if torch.cuda.is_available() else None
                        gc.collect()
        
    def start_realtime_inference(self, duration_seconds=60, output_path="camera_output.mp4", 
                            width=1280, height=720, fps=30, show_stats=True):
        """ì‹¤ì‹œê°„ ì¶”ë¡  ì‹œì‘ - ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì—†ì´"""
        
        print(f"\n=== Starting Real-time Inference ===")
        print(f"Duration: {duration_seconds} seconds")
        print(f"Output: {output_path}")
        print(f"Resolution: {width}x{height}")
        print(f"Output FPS: {fps}")
        
        # ì¹´ë©”ë¼ ì„¤ì •
        cap = self.setup_camera(width, height, fps=60)  # 60FPS ì…ë ¥
        if cap is None:
            print("âœ— Camera setup failed - exiting")
            return False
        
        # ì¶œë ¥ ë¹„ë””ì˜¤ ì„¤ì •
        # ë°”ë¡œ ëª©í‘œ FPSë¡œ ì¶œë ¥ ì„¤ì •
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        # ìŠ¤ë ˆë“œ ì‹œì‘
        capture_thread = threading.Thread(target=self.camera_capture_thread, args=(cap,))
        # ë…¹í™” ìŠ¤ë ˆë“œ ì‹œì‘
        inference_thread = threading.Thread(
            target=self.simple_realtime_thread, 
            args=(out, show_stats, fps)
        )
        
        capture_thread.start()
        inference_thread.start()
        
        try:
            print(f"Recording for {duration_seconds} seconds...")
            time.sleep(duration_seconds)
            
        except KeyboardInterrupt:
            print("\nStopping by user request...")
        
        finally:
            # ì •ë¦¬
            print("Cleaning up...")
            self.stop_processing.set()
            
            capture_thread.join(timeout=2.0)
            inference_thread.join(timeout=2.0)
            
            cap.release()
            out.release()
            
            print(f"âœ“ Video saved to: {output_path}")
            return True

    def simple_realtime_thread(self, output_writer, show_stats, target_fps):
        """ë‹¨ìˆœí•œ ì‹¤ì‹œê°„ ìŠ¤ë ˆë“œ - ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì—†ìŒ"""
        print("Starting simple realtime inference...")
        
        inference_times = []
        start_time = time.time()
        last_mask = None
        frames_written = 0
        
        with torch.inference_mode():
            while not self.stop_processing.is_set():
                # í˜„ì¬ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì¨ì•¼ í•  í”„ë ˆì„ ìˆ˜
                elapsed = time.time() - start_time
                target_frames = int(elapsed * target_fps)
                
                # ìƒˆ ì¶”ë¡  ê²°ê³¼ í™•ì¸
                if self.frame_available.wait(timeout=0.1):
                    with self.is_processing:
                        if self.latest_frame is not None:
                            current_frame = self.latest_frame.copy()
                            self.frame_available.clear()
                            
                            # ì¶”ë¡ 
                            inf_start = time.time()
                            input_tensor = self.preprocess_frame(current_frame)
                            output = self.model(input_tensor)
                            mask = self.postprocess_output(output, current_frame.shape[:2])
                            inf_time = time.time() - inf_start
                            inference_times.append(inf_time)
                            
                            # ì‹œê°í™”
                            last_mask = self.create_mask_visualization(mask)
                            
                            # ì„±ëŠ¥ í‘œì‹œ
                            if show_stats and last_mask is not None:
                                current_fps = 1 / inf_time
                                avg_fps = len(inference_times) / sum(inference_times)
                                
                                cv2.putText(last_mask, f"Current: {current_fps:.1f} | Avg: {avg_fps:.1f} FPS", 
                                        (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                                cv2.putText(last_mask, f"Model: {self.model_name}", 
                                        (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                
                # ëª©í‘œ í”„ë ˆì„ ìˆ˜ë§Œí¼ ì±„ìš°ê¸°
                while frames_written < target_frames:
                    if last_mask is not None:
                        # ë¹„ë””ì˜¤ ì €ì¥
                        output_writer.write(last_mask)
                    else:
                        # ì²« í”„ë ˆì„ ì—†ìœ¼ë©´ ê²€ì€ í™”ë©´
                        black = np.zeros((720, 1280, 3), dtype=np.uint8)
                        output_writer.write(black)
                    frames_written += 1
                
                # ì£¼ê¸°ì  í†µê³„
                if len(inference_times) % 50 == 0 and len(inference_times) > 0:
                    avg_fps = len(inference_times) / sum(inference_times)
                    print(f"Inference count: {len(inference_times)}, Avg FPS: {avg_fps:.2f}, Frames written: {frames_written}")

def main():
    parser = argparse.ArgumentParser(description="Real-time EfficientViT Camera Inference")
    parser.add_argument("--model", "-m", default="efficientvit_seg_l1", 
                       choices=list(EfficientViTModelManager.AVAILABLE_MODELS.keys()),
                       help="EfficientViT model name")
    parser.add_argument("--output", "-o", default=None, help="Output video path")
    parser.add_argument("--duration", "-d", type=int, default=60, help="Recording duration in seconds")
    parser.add_argument("--width", "-w", type=int, default=1280, help="Camera width")
    parser.add_argument("--height", type=int, default=720, help="Camera height")
    parser.add_argument("--fps", type=int, default=30, help="Output video FPS")
    parser.add_argument("--device", default="cuda", help="Device (cuda/cpu)")
    parser.add_argument("--no-optimize", action="store_true", help="Skip Jetson optimization")
    parser.add_argument("--no-stats", action="store_true", help="Hide performance overlay")
    parser.add_argument("--list-models", action="store_true", help="List available models")
    
    args = parser.parse_args()
    
    # ëª¨ë¸ ëª©ë¡ ì¶œë ¥
    if args.list_models:
        EfficientViTModelManager.list_models()
        return
    
    # ì¶œë ¥ íŒŒì¼ëª… ìë™ ìƒì„±
    if args.output is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        args.output = f"camera_inference_{args.model}_{timestamp}.mp4"
    
    try:
        # ì¶”ë¡  ê°ì²´ ìƒì„±
        inferencer = RealTimeCameraInference(
            model_name=args.model,
            device=args.device,
            optimize_jetson=not args.no_optimize
        )
        
        # ì‹¤ì‹œê°„ ì¶”ë¡  ì‹œì‘
        inferencer.start_realtime_inference(
            duration_seconds=args.duration,
            output_path=args.output,
            width=args.width,
            height=args.height,
            fps=args.fps,
            show_stats=not args.no_stats
        )
        
    except Exception as e:
        print(f"Error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()