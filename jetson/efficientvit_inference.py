import cv2
import torch
import numpy as np
import time
from pathlib import Path
import argparse
from tqdm import tqdm
import threading
import queue
import gc
import psutil
import subprocess
import os
import colorsys

# ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ ë¶€ëª¨ í´ë”(guideWay)ë¡œ ë³€ê²½ 
import sys 
script_dir = os.path.dirname(os.path.abspath(__file__)) # jetson í´ë” 
parent_dir = os.path.dirname(script_dir) # guideWay í´ë” 
os.chdir(parent_dir) # ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ guideWayë¡œ ë³€ê²½ 
print(f"Working directory changed to: {os.getcwd()}") 

# sys.pathëŠ” ì´ë¯¸ ìƒìœ„ í´ë”ë¥¼ í¬í•¨í•˜ë„ë¡ ì„¤ì • 
sys.path.insert(0, parent_dir) 

# ì´ì œ efficientvitë¥¼ ë°”ë¡œ import ê°€ëŠ¥
from efficientvit.models.efficientvit.seg import EfficientViTSeg
import torchvision.transforms as transforms

class JetsonOptimizer:
    """ì ¯ìŠ¨ í•˜ë“œì›¨ì–´ ìµœì í™” í´ë˜ìŠ¤"""
    
    @staticmethod
    def set_max_performance():
        """ì ¯ìŠ¨ì„ ìµœëŒ€ ì„±ëŠ¥ ëª¨ë“œë¡œ ì„¤ì •"""
        try:
            print("Setting Jetson to maximum performance mode...")
            
            # ìµœëŒ€ ì„±ëŠ¥ ëª¨ë“œ ì„¤ì •
            subprocess.run(['sudo', 'nvpmodel', '-m', '0'], check=True)
            print("âœ“ Power mode set to maximum")
            
            # í´ëŸ­ ìµœëŒ€í™”
            subprocess.run(['sudo', 'jetson_clocks'], check=True)
            print("âœ“ Clocks maximized")
            
            # GPU ê±°ë²„ë„ˆ ì„¤ì •
            gpu_gov_path = '/sys/devices/gpu.0/devfreq/17000000.gv11b/governor'
            if os.path.exists(gpu_gov_path):
                subprocess.run(['sudo', 'sh', '-c', f'echo performance > {gpu_gov_path}'], check=True)
                print("âœ“ GPU governor set to performance")
                
            # CPU ê±°ë²„ë„ˆ ì„¤ì •
            cpu_count = psutil.cpu_count()
            for i in range(cpu_count):
                cpu_gov_path = f'/sys/devices/system/cpu/cpu{i}/cpufreq/scaling_governor'
                if os.path.exists(cpu_gov_path):
                    subprocess.run(['sudo', 'sh', '-c', f'echo performance > {cpu_gov_path}'], check=True)
            print(f"âœ“ CPU governors set to performance for {cpu_count} cores")
            
        except subprocess.CalledProcessError as e:
            print(f"Warning: Some optimization commands failed: {e}")
        except Exception as e:
            print(f"Warning: Optimization setup failed: {e}")
    
    @staticmethod
    def get_system_info():
        """ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥"""
        try:
            # GPU ë©”ëª¨ë¦¬ ì •ë³´
            if torch.cuda.is_available():
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                print(f"GPU Memory: {gpu_memory:.1f} GB")
                
            # CPU ì •ë³´
            cpu_count = psutil.cpu_count()
            memory = psutil.virtual_memory().total / (1024**3)
            print(f"CPU Cores: {cpu_count}")
            print(f"RAM: {memory:.1f} GB")
            
            # ì ¯ìŠ¨ ëª¨ë¸ í™•ì¸
            try:
                with open('/proc/device-tree/model', 'r') as f:
                    model = f.read().strip()
                    print(f"Jetson Model: {model}")
            except:
                print("Jetson Model: Unknown")
                
        except Exception as e:
            print(f"System info gathering failed: {e}")

class EfficientViTModelManager:
    """EfficientViT ëª¨ë¸ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    AVAILABLE_MODELS = {
        'efficientvit_seg_b0': {
            'input_size': (512, 512),
            'description': 'Smallest, fastest model - good for real-time',
            'params': '0.7M'
        },
        'efficientvit_seg_b1': {
            'input_size': (512, 512), 
            'description': 'Balanced speed and accuracy',
            'params': '4.8M'
        },
        'efficientvit_seg_b2': {
            'input_size': (512, 512),
            'description': 'Higher accuracy, moderate speed',
            'params': '15.1M'
        },
        'efficientvit_seg_b3': {
            'input_size': (512, 512),
            'description': 'Best accuracy, slower inference',
            'params': '39.8M'
        },
        'efficientvit_seg_l1': {
            'input_size': (512, 512),
            'description': 'Large model - highest accuracy',
            'params': '40.5M'
        },
        'efficientvit_seg_l2': {
            'input_size': (512, 512),
            'description': 'Largest model - best quality',
            'params': '74.8M'
        }
    }
    
    @classmethod
    def list_models(cls):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¶œë ¥"""
        print("\n=== Available EfficientViT Models ===")
        for model_name, info in cls.AVAILABLE_MODELS.items():
            print(f"{model_name}:")
            print(f"  - Parameters: {info['params']}")
            print(f"  - Description: {info['description']}")
            print(f"  - Input size: {info['input_size']}")
            print()
    
    @classmethod
    def get_model_info(cls, model_name):
        """íŠ¹ì • ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return cls.AVAILABLE_MODELS.get(model_name, None)

class OptimizedEfficientViTInference:
    def __init__(self, model_name="efficientvit_seg_b0", device="cuda", optimize_jetson=True, 
                 class_mapping="auto"):
        print(f"\n=== Initializing EfficientViT Inference ===")
        
        # í‘œì¤€ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜
        self.dataset_classes = {
            'cityscapes': [
                'road', 'sidewalk', 'building', 'wall', 'fence', 'pole', 'traffic_light',
                'traffic_sign', 'vegetation', 'terrain', 'sky', 'person', 'rider', 'car',
                'truck', 'bus', 'train', 'motorcycle', 'bicycle'
            ],
            'ade20k': [
                'wall', 'building', 'sky', 'floor', 'tree', 'ceiling', 'road', 'bed',
                'windowpane', 'grass', 'cabinet', 'sidewalk', 'person', 'earth', 'door',
                'table', 'mountain', 'plant', 'curtain', 'chair', 'car', 'water',
                'painting', 'sofa', 'shelf', 'house', 'sea', 'mirror', 'rug', 'field',
                'armchair', 'seat', 'fence', 'desk', 'rock', 'wardrobe', 'lamp',
                'bathtub', 'railing', 'cushion', 'base', 'box', 'column', 'signboard',
                'chest_of_drawers', 'counter', 'sand', 'sink', 'skyscraper', 'fireplace',
                'refrigerator', 'grandstand', 'path', 'stairs', 'runway', 'case',
                'pool_table', 'pillow', 'screen_door', 'stairway', 'river', 'bridge',
                'bookcase', 'blind', 'coffee_table', 'toilet', 'flower', 'book',
                'hill', 'bench', 'countertop', 'stove', 'palm', 'kitchen_island',
                'computer', 'swivel_chair', 'boat', 'bar', 'arcade_machine',
                'hovel', 'bus', 'towel', 'light', 'truck', 'tower', 'chandelier',
                'awning', 'streetlight', 'booth', 'television_receiver', 'airplane',
                'dirt_track', 'apparel', 'pole', 'land', 'bannister', 'escalator',
                'ottoman', 'bottle', 'buffet', 'poster', 'stage', 'van', 'ship',
                'fountain', 'conveyer_belt', 'canopy', 'washer', 'plaything',
                'swimming_pool', 'stool', 'barrel', 'basket', 'waterfall', 'tent',
                'bag', 'minibike', 'cradle', 'oven', 'ball', 'food', 'step', 'tank',
                'trade_name', 'microwave', 'pot', 'animal', 'bicycle', 'lake',
                'dishwasher', 'screen', 'blanket', 'sculpture', 'hood', 'sconce',
                'vase', 'traffic_light', 'tray', 'ashcan', 'fan', 'pier', 'crt_screen',
                'plate', 'monitor', 'bulletin_board', 'shower', 'radiator', 'glass',
                'clock', 'flag'
            ],
            'pascal_voc': [
                'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',
                'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse',
                'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'
            ],
            'custom_walkway': [
                'sidewalk', 'curb-cut', 'crosswalk', 'road', 'minor-road', 'bike-lane', 'curb', 
                'terrain', 'car', 'truck', 'bus', 'motorcycle', 'bicycle', 'person', 'rider', 
                'vegetation', 'sky', 'water', 'sign', 'puddle', 'pothole', 'manhole', 'bench', 
                'pole', 'building', 'wall', 'fence'
            ]
        }
        
        # í´ë˜ìŠ¤ ë§¤í•‘ ë°©ì‹ ì„¤ì •
        self.class_mapping_type = class_mapping
        self.num_classes = None
        self.class_names = None
        self.class_colors = None
        
        # ì ¯ìŠ¨ ìµœì í™”
        if optimize_jetson:
            JetsonOptimizer.set_max_performance()
            JetsonOptimizer.get_system_info()
        
        # ëª¨ë¸ ì •ë³´ í™•ì¸
        model_info = EfficientViTModelManager.get_model_info(model_name)
        if model_info is None:
            print(f"Warning: Unknown model {model_name}")
            EfficientViTModelManager.list_models()
            raise ValueError(f"Model {model_name} not supported")
        
        self.model_name = model_name
        self.model_info = model_info
        self.device = device if torch.cuda.is_available() else "cpu"
        
        print(f"Selected Model: {model_name}")
        print(f"Model Info: {model_info['description']}")
        print(f"Parameters: {model_info['params']}")
        print(f"Device: {self.device}")
        
        # CUDA ìµœì í™” ì„¤ì •
        if torch.cuda.is_available():
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            print("âœ“ CUDA optimizations enabled")
        
        # ëª¨ë¸ ë¡œë“œ
        self.model = self.load_model()
        
        # ëª¨ë¸ì˜ ì‹¤ì œ í´ë˜ìŠ¤ ìˆ˜ ê°ì§€ ë° í´ë˜ìŠ¤ ë§¤í•‘ ì„¤ì •
        self.detect_and_setup_classes()
        
        # ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
        input_size = model_info['input_size']
        self.transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize(input_size),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        # ë©€í‹°ìŠ¤ë ˆë”©ì„ ìœ„í•œ í
        self.frame_queue = queue.Queue(maxsize=5)
        self.result_queue = queue.Queue(maxsize=5)
        
        print("âœ“ Initialization completed\n")
        
    def detect_and_setup_classes(self):
        """ëª¨ë¸ì˜ ì‹¤ì œ í´ë˜ìŠ¤ ìˆ˜ë¥¼ ê°ì§€í•˜ê³  ì ì ˆí•œ í´ë˜ìŠ¤ ë§¤í•‘ ì„¤ì •"""
        print("Detecting model output classes...")
        
        # ë”ë¯¸ ì…ë ¥ìœ¼ë¡œ ëª¨ë¸ ì¶œë ¥ í˜•íƒœ í™•ì¸
        dummy_input = torch.randn(1, 3, *self.model_info['input_size']).to(self.device)
        
        with torch.no_grad():
            dummy_output = self.model(dummy_input)
            
        # ì¶œë ¥ ì°¨ì›ì—ì„œ í´ë˜ìŠ¤ ìˆ˜ ì¶”ì¶œ
        if isinstance(dummy_output, dict):
            # ì¼ë¶€ ëª¨ë¸ì€ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì¶œë ¥
            for key in ['out', 'seg', 'logits']:
                if key in dummy_output:
                    self.num_classes = dummy_output[key].shape[1]
                    break
        else:
            # ì¼ë°˜ì ì¸ í…ì„œ ì¶œë ¥
            self.num_classes = dummy_output.shape[1]
        
        print(f"Detected {self.num_classes} output classes from model")
        
        # í´ë˜ìŠ¤ ë§¤í•‘ ê²°ì •
        if self.class_mapping_type == "auto":
            self.auto_detect_dataset()
        elif self.class_mapping_type in self.dataset_classes:
            self.class_names = self.dataset_classes[self.class_mapping_type]
            print(f"Using {self.class_mapping_type} class mapping")
        else:
            print(f"Unknown class mapping: {self.class_mapping_type}, using auto detection")
            self.auto_detect_dataset()
        
        # í´ë˜ìŠ¤ ìˆ˜ ë¶ˆì¼ì¹˜ ì‹œ ì¡°ì •
        if len(self.class_names) != self.num_classes:
            print(f"Warning: Model outputs {self.num_classes} classes, but mapping has {len(self.class_names)} classes")
            self.adjust_class_mapping()
        
        # ìƒ‰ìƒ íŒ”ë ˆíŠ¸ ìƒì„±
        self.class_colors = self.generate_color_palette(self.num_classes)
        
        print(f"âœ“ Using {len(self.class_names)} classes: {self.class_names[:5]}{'...' if len(self.class_names) > 5 else ''}")
    
    def auto_detect_dataset(self):
        """í´ë˜ìŠ¤ ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„°ì…‹ ìë™ ê°ì§€"""
        if self.num_classes == 19:
            self.class_names = self.dataset_classes['cityscapes']
            print("Auto-detected: Cityscapes dataset (19 classes)")
        elif self.num_classes == 21:
            self.class_names = self.dataset_classes['pascal_voc']
            print("Auto-detected: Pascal VOC dataset (21 classes)")
        elif self.num_classes == 150:
            self.class_names = self.dataset_classes['ade20k']
            print("Auto-detected: ADE20K dataset (150 classes)")
        elif self.num_classes == 27:
            self.class_names = self.dataset_classes['custom_walkway']
            print("Auto-detected: Custom walkway dataset (27 classes)")
        else:
            # ì¼ë°˜ì ì¸ í´ë˜ìŠ¤ ì´ë¦„ ìƒì„±
            self.class_names = [f'class_{i}' for i in range(self.num_classes)]
            print(f"Unknown dataset: Generated generic class names for {self.num_classes} classes")
    
    def adjust_class_mapping(self):
        """í´ë˜ìŠ¤ ìˆ˜ ë¶ˆì¼ì¹˜ ì‹œ ë§¤í•‘ ì¡°ì •"""
        if len(self.class_names) > self.num_classes:
            # í´ë˜ìŠ¤ ì´ë¦„ì´ ë” ë§ì€ ê²½ìš° ìë¦„
            self.class_names = self.class_names[:self.num_classes]
            print(f"Truncated class names to {self.num_classes}")
        else:
            # í´ë˜ìŠ¤ ì´ë¦„ì´ ì ì€ ê²½ìš° generic ì´ë¦„ ì¶”ê°€
            for i in range(len(self.class_names), self.num_classes):
                self.class_names.append(f'class_{i}')
            print(f"Extended class names to {self.num_classes}")
    
    def generate_color_palette(self, num_classes):
        """í´ë˜ìŠ¤ ìˆ˜ì— ë§ëŠ” ìƒ‰ìƒ íŒ”ë ˆíŠ¸ ìƒì„±"""
        # ë¯¸ë¦¬ ì •ì˜ëœ ìƒ‰ìƒë“¤ (ì£¼ìš” í´ë˜ìŠ¤ìš©)
        predefined_colors = [
            [128, 64, 128],   # road/sidewalk - ë³´ë¼
            [244, 35, 232],   # person - í•‘í¬  
            [70, 70, 70],     # building - ì§„íšŒìƒ‰
            [102, 102, 156],  # wall - ì—°ë³´ë¼
            [190, 153, 153],  # fence - ì—°ê°ˆìƒ‰
            [153, 153, 153],  # pole - íšŒìƒ‰
            [250, 170, 30],   # traffic light - ì£¼í™©
            [220, 220, 0],    # traffic sign - ë…¸ë‘
            [107, 142, 35],   # vegetation - ì˜¬ë¦¬ë¸Œ
            [152, 251, 152],  # terrain - ì—°ë…¹ìƒ‰
            [70, 130, 180],   # sky - í•˜ëŠ˜ìƒ‰
            [220, 20, 60],    # person - ë¹¨ê°•
            [255, 0, 0],      # rider - ë°ì€ë¹¨ê°•
            [0, 0, 142],      # car - íŒŒë‘
            [0, 0, 70],       # truck - ì§„íŒŒë‘
            [0, 60, 100],     # bus - ì²­ë¡
            [0, 80, 100],     # train - ì²­ë¡
            [0, 0, 230],      # motorcycle - ë°ì€íŒŒë‘
            [119, 11, 32],    # bicycle - ì ê°ˆìƒ‰
        ]
        
        colors = []
        
        # ë¯¸ë¦¬ ì •ì˜ëœ ìƒ‰ìƒ ì‚¬ìš©
        for i in range(min(num_classes, len(predefined_colors))):
            colors.append(predefined_colors[i])
        
        # ë¶€ì¡±í•œ ìƒ‰ìƒì€ HSVë¡œ ìë™ ìƒì„±
        if num_classes > len(predefined_colors):
            import colorsys
            for i in range(len(predefined_colors), num_classes):
                # HSV ìƒ‰ìƒ ê³µê°„ì—ì„œ ê· ë“±í•˜ê²Œ ë¶„í¬ëœ ìƒ‰ìƒ ìƒì„±
                hue = (i * 137.508) % 360  # í™©ê¸ˆê° ì‚¬ìš©ìœ¼ë¡œ ê· ë“± ë¶„í¬
                saturation = 0.7 + (i % 3) * 0.1  # 0.7-0.9
                value = 0.8 + (i % 2) * 0.2  # 0.8-1.0
                
                rgb = colorsys.hsv_to_rgb(hue/360, saturation, value)
                colors.append([int(rgb[0]*255), int(rgb[1]*255), int(rgb[2]*255)])
        
        return np.array(colors, dtype=np.uint8)
            
    def load_model(self):
        """EfficientViT ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ - ì˜¬ë°”ë¥¸ API ì‚¬ìš©"""
        print(f"ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_name}...")
        
        # model = None
        
        # ë°©ë²• 1: ì˜¬ë°”ë¥¸ EfficientViT API ì‚¬ìš©
        try:
            # print("EfficientViT seg_model_zooë¡œ ë¡œë”© ì¤‘...")
            # # ì˜¬ë°”ë¥¸ import ë°©ë²•
            # from efficientvit.seg_model_zoo import create_efficientvit_seg_model
            
            # # # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ ì„¤ì •
            # # checkpoint_path = "efficientvit/assets/checkpoints/efficientvit_seg/efficientvit_seg_b0_cityscapes.pt"
            
            # # # ëª¨ë¸ëª… ë§¤í•‘ (ì°¸ê³  ì½”ë“œ ê¸°ë°˜)
            # # model_mapping = {
            # #     'efficientvit_seg_b0': 'efficientvit-seg-b0',  # ìˆ˜ì •ëœ ë¶€ë¶„
            # #     'efficientvit_seg_b1': 'efficientvit-seg-b1', 
            # #     'efficientvit_seg_b2': 'efficientvit-seg-b2',
            # #     'efficientvit_seg_b3': 'efficientvit-seg-b3',
            # #     'efficientvit_seg_l1': 'efficientvit-seg-l1',
            # #     'efficientvit_seg_l2': 'efficientvit-seg-l2'
            # # }
            
            # # model_name_mapped = model_mapping.get(self.model_name, 'efficientvit-seg-b0')
            
            # # # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ë¡œì»¬ì—ì„œ ë¡œë“œ
            # # if os.path.exists(checkpoint_path):
            # #     print(f"âœ“ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ë°œê²¬: {checkpoint_path}")
            # #     # ìˆ˜ì •ëœ í•¨ìˆ˜ í˜¸ì¶œ ë°©ì‹ (ì´ë¯¸ì§€ ì°¸ê³ )
            # #     model = create_efficientvit_seg_model(
            # #         name=model_name_mapped,
            # #         dataset="cityscapes",  # í•„ìˆ˜ ì¸ì ì¶”ê°€
            # #         weight_url=checkpoint_path,
            # #         n_classes=19  # Cityscapes í´ë˜ìŠ¤ ìˆ˜
            # #     )
            # #     print(f"âœ“ ë¡œì»¬ ì²´í¬í¬ì¸íŠ¸ì—ì„œ EfficientViT ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_name_mapped}")
            # # else:
            # #     # ì˜¨ë¼ì¸ì—ì„œ ë‹¤ìš´ë¡œë“œ
            # #     print("ì˜¨ë¼ì¸ì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")
            # #     model = create_efficientvit_seg_model(
            # #         name=model_name_mapped,
            # #         dataset="cityscapes",  # í•„ìˆ˜ ì¸ì ì¶”ê°€
            # #         pretrained=True,
            # #         n_classes=19  # Cityscapes í´ë˜ìŠ¤ ìˆ˜
            # #     )
            # #     print(f"âœ“ ì˜¨ë¼ì¸ì—ì„œ EfficientViT ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_name_mapped}")
            # # âœ… ìˆ˜ì •: ëª¨ë¸ë³„ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ ë§¤í•‘
            # checkpoint_mapping = {
            #     'efficientvit_seg_b0': "efficientvit/assets/checkpoints/efficientvit_seg/efficientvit_seg_b0_cityscapes.pt",
            #     'efficientvit_seg_b1': "efficientvit/assets/checkpoints/efficientvit_seg/efficientvit_seg_b1_cityscapes.pt",
            #     'efficientvit_seg_b2': "efficientvit/assets/checkpoints/efficientvit_seg/efficientvit_seg_b2_cityscapes.pt", 
            #     'efficientvit_seg_b3': "efficientvit/assets/checkpoints/efficientvit_seg/efficientvit_seg_b3_cityscapes.pt",
            #     'efficientvit_seg_l1': "efficientvit/assets/checkpoints/efficientvit_seg/efficientvit_seg_l1_cityscapes.pt",
            #     'efficientvit_seg_l2': "efficientvit/assets/checkpoints/efficientvit_seg/efficientvit_seg_l2_cityscapes.pt"
            # }
            
            # checkpoint_path = checkpoint_mapping.get(self.model_name)
            from efficientvit.seg_model_zoo import create_efficientvit_seg_model, REGISTERED_EFFICIENTVIT_SEG_MODEL
            
            # ëª¨ë¸ëª… ë§¤í•‘ (ìˆ˜ì •ëœ ë¶€ë¶„)
            model_mapping = {
                'efficientvit_seg_b0': 'efficientvit-seg-b0',
                'efficientvit_seg_b1': 'efficientvit-seg-b1', 
                'efficientvit_seg_b2': 'efficientvit-seg-b2',
                'efficientvit_seg_b3': 'efficientvit-seg-b3',
                'efficientvit_seg_l1': 'efficientvit-seg-l1',
                'efficientvit_seg_l2': 'efficientvit-seg-l2'  # âœ… l2 ë§¤í•‘ ì¶”ê°€
            }
            
            model_name_mapped = model_mapping.get(self.model_name, 'efficientvit-seg-b0')
            
        #     # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ë¡œì»¬ì—ì„œ ë¡œë“œ
        #     if checkpoint_path and os.path.exists(checkpoint_path):
        #         print(f"âœ“ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ë°œê²¬: {checkpoint_path}")
        #         model = create_efficientvit_seg_model(
        #             name=model_name_mapped,
        #             dataset="cityscapes",
        #             weight_url=checkpoint_path,
        #             n_classes=19
        #         )
        #         print(f"âœ“ ë¡œì»¬ ì²´í¬í¬ì¸íŠ¸ì—ì„œ EfficientViT ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_name_mapped}")
        #     else:
        #         # âœ… ìˆ˜ì •: ì˜¨ë¼ì¸ì—ì„œ ë‹¤ìš´ë¡œë“œ (ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì´ ì—†ì„ ë•Œ)
        #         print("ì˜¨ë¼ì¸ì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")
        #         model = create_efficientvit_seg_model(
        #             name=model_name_mapped,
        #             dataset="cityscapes",
        #             pretrained=True,
        #             n_classes=19
        #         )
        #         print(f"âœ“ ì˜¨ë¼ì¸ì—ì„œ EfficientViT ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_name_mapped}")
                
        # except ImportError as e:
        #     print(f"EfficientViT import ì‹¤íŒ¨: {e}")
        # except Exception as e:
        #     print(f"EfficientViT ë¡œë”© ì‹¤íŒ¨: {e}")
        
        # # ë°©ë²• 2: ì»¤ìŠ¤í…€ ëª¨ë¸ ê²½ë¡œ ì‹œë„ (ì´ë¯¸ì§€ì—ì„œ ë³´ì¸ ê²½ë¡œ)
        # if model is None:
        #     try:
        #         print("ì»¤ìŠ¤í…€ ëª¨ë¸ ê²½ë¡œë¡œ ì‹œë„ ì¤‘...")
        #         from efficientvit.seg_model_zoo import create_efficientvit_seg_model
                
        #         # ì´ë¯¸ì§€ì—ì„œ ë³¸ ê²½ë¡œ ì‚¬ìš©
        #         custom_weight_path = "D:/Aiffel/efficientvit/efficientvit/output/from_runpod/model_best(1).pt"
                
        #         if os.path.exists(custom_weight_path):
        #             model = create_efficientvit_seg_model(
        #                 name="efficientvit-seg-b0",
        #                 dataset="cityscapes",
        #                 weight_url=custom_weight_path,
        #                 n_classes=18  # ì´ë¯¸ì§€ì—ì„œ ë³¸ í´ë˜ìŠ¤ ìˆ˜
        #             )
        #             print(f"âœ“ ì»¤ìŠ¤í…€ ê²½ë¡œì—ì„œ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        #         else:
        #             print(f"ì»¤ìŠ¤í…€ ê°€ì¤‘ì¹˜ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {custom_weight_path}")
                    
        #     except Exception as e:
        #         print(f"ì»¤ìŠ¤í…€ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        
        # # ë°©ë²• 3: ê¸°ë³¸ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ì‹œë„
        # if model is None:
        #     try:
        #         print("ê¸°ë³¸ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ë¡œ ì‹œë„ ì¤‘...")
        #         from efficientvit.seg_model_zoo import create_efficientvit_seg_model
                
        #         model = create_efficientvit_seg_model(
        #             name="efficientvit-seg-b0",
        #             dataset="cityscapes",
        #             pretrained=True
        #         )
        #         print("âœ“ ê¸°ë³¸ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
                
        #     except Exception as e:
        #         print(f"ê¸°ë³¸ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        
        # # # ë°©ë²• 4: HuggingFace SegFormer ëŒ€ì•ˆ
        # # if model is None:
        # #     try:
        # #         print("ëŒ€ì•ˆìœ¼ë¡œ HuggingFaceì—ì„œ SegFormer ë¡œë”© ì¤‘...")
        # #         from transformers import SegformerForSemanticSegmentation
                
        # #         model = SegformerForSemanticSegmentation.from_pretrained(
        # #             "nvidia/segformer-b0-finetuned-cityscapes-512-1024"
        # #         )
        # #         print("âœ“ HuggingFaceì—ì„œ SegFormer ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
                
        # #     except Exception as e:
        # #         print(f"HuggingFace SegFormer ì‹¤íŒ¨: {e}")
        
        # # # ë°©ë²• 5: ìµœì†Œ ì‘ë™ ëª¨ë¸ (ìµœí›„ì˜ ìˆ˜ë‹¨)
        # # if model is None:
        # #     print("ìµœì†Œ ì‘ë™ ëª¨ë¸ ìƒì„± ì¤‘...")
            
        # #     import torch.nn as nn
        # #     import torch.nn.functional as F
            
        # #     class MinimalSegmentationModel(nn.Module):
        # #         def __init__(self, num_classes=19):
        # #             super().__init__()
        # #             self.num_classes = num_classes
                    
        # #             # ê°„ë‹¨í•œ encoder-decoder êµ¬ì¡°
        # #             self.encoder = nn.Sequential(
        # #                 nn.Conv2d(3, 64, 3, padding=1),
        # #                 nn.ReLU(inplace=True),
        # #                 nn.Conv2d(64, 64, 3, padding=1),
        # #                 nn.ReLU(inplace=True),
        # #                 nn.MaxPool2d(2),
                        
        # #                 nn.Conv2d(64, 128, 3, padding=1),
        # #                 nn.ReLU(inplace=True),
        # #                 nn.Conv2d(128, 128, 3, padding=1),
        # #                 nn.ReLU(inplace=True),
        # #                 nn.MaxPool2d(2),
                        
        # #                 nn.Conv2d(128, 256, 3, padding=1),
        # #                 nn.ReLU(inplace=True),
        # #             )
                    
        # #             # Decoder
        # #             self.decoder = nn.Sequential(
        # #                 nn.Conv2d(256, 128, 3, padding=1),
        # #                 nn.ReLU(inplace=True),
        # #                 nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),
                        
        # #                 nn.Conv2d(128, 64, 3, padding=1),
        # #                 nn.ReLU(inplace=True),
        # #                 nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),
                        
        # #                 nn.Conv2d(64, num_classes, 1)
        # #             )
                
        # #         def forward(self, x):
        # #             features = self.encoder(x)
        # #             output = self.decoder(features)
                    
        # #             # ì…ë ¥ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
        # #             output = F.interpolate(output, size=x.shape[2:], mode='bilinear', align_corners=False)
        # #             return output
            
        # #     model = MinimalSegmentationModel(num_classes=19)
        # #     print("âœ“ ìµœì†Œ ì‘ë™ ëª¨ë¸ ìƒì„± ì™„ë£Œ")
        # #     print("âš ï¸ ê²½ê³ : í›ˆë ¨ë˜ì§€ ì•Šì€ ìµœì†Œ ëª¨ë¸ì„ ë°ëª¨ìš©ìœ¼ë¡œ ì‚¬ìš©")
            # ğŸ”„ ë™ì  ê²½ë¡œ í•´ê²°: seg_model_zooì˜ ë“±ë¡ ì •ë³´ í™œìš©
            if model_name_mapped in REGISTERED_EFFICIENTVIT_SEG_MODEL:
                model_builder, norm_eps, registered_path = REGISTERED_EFFICIENTVIT_SEG_MODEL[model_name_mapped]
                
                # efficientvit í•˜ìœ„ ë””ë ‰í† ë¦¬ë¥¼ ê³ ë ¤í•œ ê²½ë¡œ ìƒì„±
                current_dir = os.getcwd()
                efficientvit_path = os.path.join(current_dir, "efficientvit", registered_path)
                
                print(f"ë“±ë¡ëœ ê²½ë¡œ: {registered_path}")
                print(f"ì‹¤ì œ í™•ì¸ ê²½ë¡œ: {efficientvit_path}")
                
                if os.path.exists(efficientvit_path):
                    print(f"âœ“ ë¡œì»¬ ì²´í¬í¬ì¸íŠ¸ ë°œê²¬: {efficientvit_path}")
                    model = create_efficientvit_seg_model(
                        name=model_name_mapped,
                        dataset="cityscapes",
                        weight_url=efficientvit_path,
                        n_classes=19
                    )
                    print(f"âœ“ ë¡œì»¬ì—ì„œ ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_name_mapped}")
                else:
                    print(f"ë¡œì»¬ íŒŒì¼ ì—†ìŒ, ì˜¨ë¼ì¸ ë‹¤ìš´ë¡œë“œ ì‹œë„...")
                    model = create_efficientvit_seg_model(
                        name=model_name_mapped,
                        dataset="cityscapes",
                        pretrained=True,
                        weight_url=None,  # seg_model_zooê°€ ì•Œì•„ì„œ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
                        n_classes=19
                    )
                    print(f"âœ“ ì˜¨ë¼ì¸ì—ì„œ ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_name_mapped}")
            else:
                # ë“±ë¡ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° ì˜¨ë¼ì¸ ë‹¤ìš´ë¡œë“œ
                print(f"ë“±ë¡ ì •ë³´ ì—†ìŒ, ì˜¨ë¼ì¸ ë‹¤ìš´ë¡œë“œ...")
                model = create_efficientvit_seg_model(
                    name=model_name_mapped,
                    dataset="cityscapes",
                    pretrained=True,
                    n_classes=19
                )
                print(f"âœ“ ì˜¨ë¼ì¸ì—ì„œ ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_name_mapped}")
                
        except Exception as e:
            print(f"EfficientViT ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
        
        if model is None:
            raise RuntimeError("ëª¨ë“  ëª¨ë¸ ë¡œë”© ë°©ë²•ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì„¤ì¹˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
        # ëª¨ë¸ ìµœì í™”
        model = model.to(self.device)
        model.eval()
        
        # TensorRT ìµœì í™”
        if self.device == "cuda" and hasattr(torch, 'jit'):
            try:
                dummy_input = torch.randn(1, 3, *self.model_info['input_size']).to(self.device)
                with torch.no_grad():
                    _ = model(dummy_input)
                
                # JIT ì»´íŒŒì¼
                model = torch.jit.trace(model, dummy_input)
                print("âœ“ TorchScriptë¡œ ëª¨ë¸ ìµœì í™” ì™„ë£Œ")
            except Exception as e:
                print(f"TorchScript ìµœì í™” ì‹¤íŒ¨: {e}")
        
        return model
    
    def preprocess_frame(self, frame):
        """ìµœì í™”ëœ í”„ë ˆì„ ì „ì²˜ë¦¬"""
        # BGR to RGB ë³€í™˜ ìµœì í™”
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        
        # ë³€í™˜ ì ìš©
        input_tensor = self.transform(frame_rgb).unsqueeze(0)
        return input_tensor.to(self.device, non_blocking=True)
    
    def postprocess_output(self, output, original_shape):
        # """ìµœì í™”ëœ ì¶œë ¥ í›„ì²˜ë¦¬"""
        # # GPUì—ì„œ ì§ì ‘ ì²˜ë¦¬
        # with torch.no_grad():
        #     # ëª¨ë¸ ì¶œë ¥ì´ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° ì²˜ë¦¬
        #     if isinstance(output, dict):
        #         for key in ['out', 'seg', 'logits']:
        #             if key in output:
        #                 logits = output[key]
        #                 break
        #         else:
        #             # ì²« ë²ˆì§¸ ê°’ ì‚¬ìš©
        #             logits = list(output.values())[0]
        #     else:
        #         logits = output
            
        #     # ì†Œí”„íŠ¸ë§¥ìŠ¤ ë° argmax
        #     probs = torch.softmax(logits, dim=1)
        #     pred = torch.argmax(probs, dim=1).squeeze()
            
        #     # CPUë¡œ ì´ë™í•˜ì—¬ ë¦¬ì‚¬ì´ì¦ˆ
        #     pred_cpu = pred.cpu().numpy().astype(np.uint8)
            
        # # ì›ë³¸ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
        # pred_resized = cv2.resize(pred_cpu, 
        #                         (original_shape[1], original_shape[0]), 
        #                         interpolation=cv2.INTER_NEAREST)
        #                         # interpolation=cv2.INTER_LINEAR)  # ë” ë¶€ë“œëŸ¬ìš´ ë³´ê°„ -> í´ë˜ìŠ¤ ê²½ê³„ì—ì„œ ì˜ëª»ëœ ì¤‘ê°„ê°’ë“¤ì´ ìƒì„±ë¨ (ì‹¤íŒ¨)
        
        # return pred_resized
        # """ë¶€ë“œëŸ¬ìš´ ì„¸ê·¸ë©˜í…Œì´ì…˜ì„ ìœ„í•œ ê°œì„ ëœ ì¶œë ¥ í›„ì²˜ë¦¬"""
        # with torch.no_grad():
        #     # ëª¨ë¸ ì¶œë ¥ ì²˜ë¦¬
        #     if isinstance(output, dict):
        #         for key in ['out', 'seg', 'logits']:
        #             if key in output:
        #                 logits = output[key]
        #                 break
        #         else:
        #             logits = list(output.values())[0]
        #     else:
        #         logits = output
            
        #     # ğŸ”¥ í•µì‹¬ ê°œì„ : GPUì—ì„œ ë°”ë¡œ ì›ë³¸ í¬ê¸°ë¡œ ì—…ìƒ˜í”Œë§
        #     upsampled_logits = torch.nn.functional.interpolate(
        #         logits, 
        #         size=original_shape[:2], 
        #         mode='bilinear', 
        #         # mode='bicubic',  # ë§ˆìŠ¤í‚¹ í€„ë¦¬í‹° ëŒ€ì•ˆ
        #         align_corners=False
        #     )
            
        #     # ì†Œí”„íŠ¸ë§¥ìŠ¤ ë° argmax
        #     probs = torch.softmax(upsampled_logits, dim=1)
        #     pred = torch.argmax(probs, dim=1).squeeze()
            
        #     # CPUë¡œ ì´ë™
        #     pred_cpu = pred.cpu().numpy().astype(np.uint8)
        """ì ¯ìŠ¨ ìµœì í™”ëœ í›„ì²˜ë¦¬"""
        # torch.no_grad() ì œê±° (ì´ë¯¸ inference_mode ì•ˆì— ìˆìŒ)
        # ëª¨ë¸ ì¶œë ¥ ì²˜ë¦¬
        if isinstance(output, dict):
            for key in ['out', 'seg', 'logits']:
                if key in output:
                    logits = output[key]
                    break
            else:
                logits = list(output.values())[0]
        else:
            logits = output
        
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë³´ê°„ ë° argmax
        if logits.shape[-2:] != original_shape[:2]:
            upsampled_logits = torch.nn.functional.interpolate(
                logits, 
                size=original_shape[:2], 
                mode='bilinear',  # ë˜ëŠ” 'bicubic'
                align_corners=False
            )
        else:
            upsampled_logits = logits
        
        # ì†Œí”„íŠ¸ë§¥ìŠ¤ ì—†ì´ ë°”ë¡œ argmax (ë©”ëª¨ë¦¬/ì—°ì‚° ì ˆì•½)
        pred = torch.argmax(upsampled_logits, dim=1).squeeze()
        return pred.cpu().numpy().astype(np.uint8)
    
    def create_mask_visualization(self, segmentation_mask):
        """ë§ˆìŠ¤í¬ë§Œìœ¼ë¡œ êµ¬ì„±ëœ ì‹œê°í™” ìƒì„±"""
        # ë²¡í„°í™”ëœ ìƒ‰ìƒ ë§¤í•‘ (27ê°œ í´ë˜ìŠ¤)
        colored_mask = self.class_colors[segmentation_mask % len(self.class_colors)]
        colored_mask = colored_mask[..., ::-1]  # RGB -> BGR ë³€í™˜
        
        # í´ë˜ìŠ¤ë³„ í”½ì…€ ìˆ˜ ê³„ì‚°
        unique, counts = np.unique(segmentation_mask, return_counts=True)
        class_info = {}
        for class_id, count in zip(unique, counts):
            if class_id < len(self.class_names):
                class_info[self.class_names[class_id]] = count
            else:
                class_info[f'unknown_{class_id}'] = count
        
        return colored_mask, class_info
    
    def create_enhanced_overlay(self, frame, segmentation_mask, alpha=0.6):
        """í–¥ìƒëœ ì˜¤ë²„ë ˆì´ ìƒì„± (ì›ë³¸ í”„ë ˆì„ + ë§ˆìŠ¤í¬)"""
        # ë²¡í„°í™”ëœ ìƒ‰ìƒ ë§¤í•‘
        colored_mask = self.class_colors[segmentation_mask % len(self.class_colors)]
        colored_mask = colored_mask[..., ::-1]  # RGB -> BGR ë³€í™˜
        
        # ë¸”ë Œë”©
        overlay = cv2.addWeighted(frame, 1-alpha, colored_mask, alpha, 0)
        
        # í´ë˜ìŠ¤ë³„ í”½ì…€ ìˆ˜ ê³„ì‚°
        unique, counts = np.unique(segmentation_mask, return_counts=True)
        class_info = {}
        for class_id, count in zip(unique, counts):
            if class_id < len(self.class_names):
                class_info[self.class_names[class_id]] = count
            else:
                class_info[f'unknown_{class_id}'] = count
        
        return overlay, class_info
    
    def save_mask_legend(self, save_dir):
        """í´ë˜ìŠ¤ë³„ ìƒ‰ìƒ ë²”ë¡€ ì €ì¥"""
        legend_height = len(self.class_names) * 30 + 50
        legend_width = 400
        legend = np.ones((legend_height, legend_width, 3), dtype=np.uint8) * 255
        
        # ì œëª©
        cv2.putText(legend, "Segmentation Classes", (10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)
        
        # ê° í´ë˜ìŠ¤ë³„ ìƒ‰ìƒê³¼ ì´ë¦„
        for i, (class_name, color) in enumerate(zip(self.class_names, self.class_colors)):
            y_pos = 60 + i * 25
            
            # ìƒ‰ìƒ ë°•ìŠ¤
            cv2.rectangle(legend, (10, y_pos - 10), (40, y_pos + 10), 
                         color.tolist(), -1)
            cv2.rectangle(legend, (10, y_pos - 10), (40, y_pos + 10), 
                         (0, 0, 0), 1)
            
            # í´ë˜ìŠ¤ ì´ë¦„
            cv2.putText(legend, f"{i}: {class_name}", (50, y_pos + 5), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)
        
        # ë²”ë¡€ ì €ì¥
        legend_path = save_dir / "class_legend.png"
        cv2.imwrite(str(legend_path), legend)
        print(f"âœ“ Class legend saved to: {legend_path}")
    
    def frame_producer(self, cap, total_frames):
        """í”„ë ˆì„ ìƒì‚°ì ìŠ¤ë ˆë“œ"""
        frame_idx = 0
        while frame_idx < total_frames:
            ret, frame = cap.read()
            if not ret:
                break
                
            try:
                self.frame_queue.put((frame_idx, frame), timeout=1.0)
                frame_idx += 1
            except queue.Full:
                continue
                
        # ì¢…ë£Œ ì‹ í˜¸
        self.frame_queue.put(None)
    
    def inference_worker(self):
        """ì¶”ë¡  ì›Œì»¤ ìŠ¤ë ˆë“œ"""
        # while True:
        #     try:
        #         item = self.frame_queue.get(timeout=1.0)
        #         if item is None:
        #             self.result_queue.put(None)
        #             break
                    
        #         frame_idx, frame = item
                
        #         # ì¶”ë¡ 
        #         start_time = time.time()
        #         input_tensor = self.preprocess_frame(frame)
                
        #         with torch.no_grad():
        #             output = self.model(input_tensor)
                
        #         segmentation_mask = self.postprocess_output(output, frame.shape[:2])
        #         inference_time = time.time() - start_time
                
        #         # ê²°ê³¼ ì „ì†¡
        #         self.result_queue.put((frame_idx, frame, segmentation_mask, inference_time))
                
        #     except queue.Empty:
        #         continue
        #     except Exception as e:
        #         print(f"Inference error: {e}")
        #         continue
        with torch.inference_mode():  # torch.no_grad() ëŒ€ì‹  ì‚¬ìš©
            while True:
                try:
                    item = self.frame_queue.get(timeout=1.0)
                    if item is None:
                        self.result_queue.put(None)
                        break
                        
                    frame_idx, frame = item
                    
                    # ì¶”ë¡ 
                    start_time = time.time()
                    input_tensor = self.preprocess_frame(frame)
                    
                    output = self.model(input_tensor)  # torch.no_grad() ì œê±° (ì´ë¯¸ inference_mode ì•ˆ)
                    
                    segmentation_mask = self.postprocess_output(output, frame.shape[:2])
                    inference_time = time.time() - start_time
                    
                    self.result_queue.put((frame_idx, frame, segmentation_mask, inference_time))
                    
                except queue.Empty:
                    continue
                except Exception as e:
                    print(f"Inference error: {e}")
                    continue
    
    def process_video_optimized(self, input_path, output_path, save_frames=False, save_masks=False,
                              show_stats=True, multithreading=True):
        # """ìµœì í™”ëœ ë¹„ë””ì˜¤ ì²˜ë¦¬"""
        # cap = cv2.VideoCapture(input_path)
        """ìµœì í™”ëœ ë¹„ë””ì˜¤ ì²˜ë¦¬ - save_masksê°€ Trueë©´ ë§ˆìŠ¤í¬ë§Œ, Falseë©´ ì˜¤ë²„ë ˆì´ë§Œ ì¶œë ¥"""
        
        # save_masks ì˜µì…˜ì— ë”°ë¼ ì¶œë ¥ ëª¨ë“œ ê²°ì •
        masks_only = save_masks
        
        cap = cv2.VideoCapture(input_path)
        
        # ë¹„ë””ì˜¤ ì •ë³´
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        print(f"\n=== Video Processing Info ===")
        print(f"Resolution: {width}x{height}")
        print(f"FPS: {fps}")
        print(f"Total frames: {total_frames}")
        print(f"Duration: {total_frames/fps:.1f} seconds")
        print(f"Multithreading: {multithreading}")
        print(f"Output mode: {'Masks only' if masks_only else 'Overlay'}")  # ğŸ”¥ ëª¨ë“œ í‘œì‹œ
        
        # ì¶œë ¥ ë¹„ë””ì˜¤ ì„¤ì •
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        # # ë§ˆìŠ¤í¬ ë¹„ë””ì˜¤ ì¶œë ¥ ì„¤ì • (save_masksê°€ Trueì¸ ê²½ìš°)
        # mask_out = None
        # if save_masks:
        #     mask_output_path = output_path.replace('.mp4', '_masks.mp4')
        #     mask_out = cv2.VideoWriter(mask_output_path, fourcc, fps, (width, height))
        #     print(f"Mask video will be saved to: {mask_output_path}")
        
        # í”„ë ˆì„/ë§ˆìŠ¤í¬ ì €ì¥ ë””ë ‰í† ë¦¬
        # if save_frames or save_masks:
        #     if save_frames:
        #         frames_dir = Path("output_frames")
        #         frames_dir.mkdir(exist_ok=True)
        #     if save_masks:
        #         masks_dir = Path("output_masks")
        #         masks_dir.mkdir(exist_ok=True)
        #         # í´ë˜ìŠ¤ ë²”ë¡€ ì €ì¥
        #         self.save_mask_legend(masks_dir)
        if save_frames:
            if masks_only:
                frames_dir = Path("output_masks")
                frames_dir.mkdir(exist_ok=True)
                self.save_mask_legend(frames_dir)
            else:
                frames_dir = Path("output_frames")
                frames_dir.mkdir(exist_ok=True)
        
        # í†µê³„ ë³€ìˆ˜
        inference_times = []
        class_statistics = {}
        
        if multithreading:
            # ë©€í‹°ìŠ¤ë ˆë“œ ì²˜ë¦¬
            producer_thread = threading.Thread(target=self.frame_producer, args=(cap, total_frames))
            inference_thread = threading.Thread(target=self.inference_worker)
            
            producer_thread.start()
            inference_thread.start()
            
            processed_frames = 0
            frame_buffer = {}  # ìˆœì„œ ë³´ì¥ì„ ìœ„í•œ ë²„í¼
            expected_frame = 0
            
            pbar = tqdm(total=total_frames, desc="Processing")
            
            try:
                while processed_frames < total_frames:
                    try:
                        result = self.result_queue.get(timeout=5.0)
                        if result is None:
                            break
                            
                        frame_idx, frame, segmentation_mask, inference_time = result
                        inference_times.append(inference_time)
                        
                        # ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ë²„í¼ì— ì €ì¥
                        frame_buffer[frame_idx] = (frame, segmentation_mask, inference_time)
                        
                        # ìˆœì„œëŒ€ë¡œ ì¶œë ¥
                        while expected_frame in frame_buffer:
                            frame, seg_mask, inf_time = frame_buffer.pop(expected_frame)
                            
                            # # ì˜¤ë²„ë ˆì´ ìƒì„± (ì›ë³¸ + ë§ˆìŠ¤í¬)
                            # overlay, class_info = self.create_enhanced_overlay(frame, seg_mask)
                            
                            # # ë§ˆìŠ¤í¬ë§Œ ìƒì„± (save_masks ì˜µì…˜)
                            # if save_masks or mask_out:
                            #     mask_only, _ = self.create_mask_visualization(seg_mask)
                            # ğŸ”¥ í•µì‹¬ ìˆ˜ì •: masks_onlyì— ë”°ë¼ ë‹¤ë¥¸ ì²˜ë¦¬
                            if masks_only:
                                # ë§ˆìŠ¤í¬ë§Œ ìƒì„±
                                # final_output, class_info = self.create_mask_visualization(seg_mask)
                                final_output, class_info = self.create_opencv_bitwise_mask_visualization(seg_mask)
                            else:
                                # ì˜¤ë²„ë ˆì´ ìƒì„± (ê¸°ì¡´ ë™ì‘)
                                final_output, class_info = self.create_enhanced_overlay(frame, seg_mask)
                            
                            # í†µê³„ ì •ë³´ ì¶”ê°€
                            for class_name, count in class_info.items():
                                if class_name not in class_statistics:
                                    class_statistics[class_name] = []
                                class_statistics[class_name].append(count)
                            
                            # # ì„±ëŠ¥ ì •ë³´ í‘œì‹œ
                            # if show_stats:
                            #     fps_text = f"FPS: {1/inf_time:.1f}"
                            #     model_text = f"Model: {self.model_name}"
                            #     cv2.putText(overlay, fps_text, (10, 30), 
                            #                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                            #     cv2.putText(overlay, model_text, (10, 60), 
                            #                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                            # ì„±ëŠ¥ ì •ë³´ í‘œì‹œ (ì˜¤ë²„ë ˆì´ ëª¨ë“œì¼ ë•Œë§Œ, ë§ˆìŠ¤í¬ ëª¨ë“œì¼ ë•ŒëŠ” í‘œì‹œí•˜ì§€ ì•ŠìŒ)
                            if show_stats and not masks_only:
                                fps_text = f"FPS: {1/inf_time:.1f}"
                                model_text = f"Model: {self.model_name}"
                                cv2.putText(final_output, fps_text, (10, 30), 
                                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                                cv2.putText(final_output, model_text, (10, 60), 
                                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                            
                            # # ë¹„ë””ì˜¤ ì €ì¥
                            # out.write(overlay)
                            # if mask_out is not None:
                            #     mask_out.write(mask_only)
                            # ğŸ”¥ ë¹„ë””ì˜¤ ì €ì¥ - ë©”ì¸ ì¶œë ¥
                            out.write(final_output)
                            
                            # ê°œë³„ í”„ë ˆì„/ë§ˆìŠ¤í¬ ì €ì¥
                            # if save_frames:
                            #     cv2.imwrite(str(frames_dir / f"frame_{expected_frame:06d}.jpg"), overlay)
                            # if save_masks:
                            #     cv2.imwrite(str(masks_dir / f"mask_{expected_frame:06d}.png"), mask_only)
                            if save_frames:
                                if masks_only:
                                    cv2.imwrite(str(frames_dir / f"mask_{expected_frame:06d}.png"), final_output)
                                else:
                                    cv2.imwrite(str(frames_dir / f"frame_{expected_frame:06d}.jpg"), final_output)
                            
                            expected_frame += 1
                            processed_frames += 1
                            pbar.update(1)
                            
                            # ë©”ëª¨ë¦¬ ê´€ë¦¬
                            if processed_frames % 30 == 0:
                                torch.cuda.empty_cache() if torch.cuda.is_available() else None
                                gc.collect()
                    
                    except queue.Empty:
                        print("Timeout waiting for results")
                        break
                        
            except KeyboardInterrupt:
                print("\nProcessing interrupted")
            
            finally:
                producer_thread.join(timeout=1.0)
                inference_thread.join(timeout=1.0)
                pbar.close()
        
        else:
            # # ë‹¨ì¼ ìŠ¤ë ˆë“œ ì²˜ë¦¬ (ì•ˆì •ì„± ìš°ì„ )# ğŸ”¥ ë‹¨ì¼ ìŠ¤ë ˆë“œ ì²˜ë¦¬ ë¶€ë¶„ë„ ë™ì¼í•˜ê²Œ ìˆ˜ì •
            # pbar = tqdm(total=total_frames, desc="Processing")
            
            # for frame_idx in range(total_frames):
            #     ret, frame = cap.read()
            #     if not ret:
            #         break
                
            #     # ì¶”ë¡ 
            #     start_time = time.time()
            #     input_tensor = self.preprocess_frame(frame)
                
            #     with torch.no_grad():
            #         output = self.model(input_tensor)
                
            #     segmentation_mask = self.postprocess_output(output, frame.shape[:2])
            #     inference_time = time.time() - start_time
            #     inference_times.append(inference_time)
                
            #     # # ì˜¤ë²„ë ˆì´ ìƒì„± (ì›ë³¸ + ë§ˆìŠ¤í¬)
            #     # overlay, class_info = self.create_enhanced_overlay(frame, segmentation_mask)
                
            #     # # ë§ˆìŠ¤í¬ë§Œ ìƒì„± (save_masks ì˜µì…˜)
            #     # if save_masks or mask_out:
            #     #     mask_only, _ = self.create_mask_visualization(segmentation_mask)
            #     # ğŸ”¥ masks_onlyì— ë”°ë¼ ë‹¤ë¥¸ ì²˜ë¦¬
            #     if masks_only:
            #         # final_output, class_info = self.create_mask_visualization(segmentation_mask)
            #         final_output, class_info = self.create_opencv_bitwise_mask_visualization(segmentation_mask)
            #     else:
            #         final_output, class_info = self.create_enhanced_overlay(frame, segmentation_mask)
                
            #     # í†µê³„ ìˆ˜ì§‘
            #     for class_name, count in class_info.items():
            #         if class_name not in class_statistics:
            #             class_statistics[class_name] = []
            #         class_statistics[class_name].append(count)
                
            #     # # ì„±ëŠ¥ ì •ë³´ í‘œì‹œ
            #     # if show_stats:
            #     #     fps_text = f"FPS: {1/inference_time:.1f}"
            #     #     model_text = f"Model: {self.model_name}"
            #     #     cv2.putText(overlay, fps_text, (10, 30), 
            #     #                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            #     #     cv2.putText(overlay, model_text, (10, 60), 
            #     #                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            #     # ì„±ëŠ¥ ì •ë³´ í‘œì‹œ (ë§ˆìŠ¤í¬ ëª¨ë“œì¼ ë•ŒëŠ” í‘œì‹œí•˜ì§€ ì•ŠìŒ)
            #     if show_stats and not masks_only:
            #         fps_text = f"FPS: {1/inference_time:.1f}"
            #         model_text = f"Model: {self.model_name}"
            #         cv2.putText(final_output, fps_text, (10, 30), 
            #                 cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            #         cv2.putText(final_output, model_text, (10, 60), 
            #                 cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                
            #     # # ë¹„ë””ì˜¤ ì €ì¥
            #     # out.write(overlay)
            #     # ğŸ”¥ ë¹„ë””ì˜¤ ì €ì¥
            #     out.write(final_output)
                
            #     # # ê°œë³„ í”„ë ˆì„/ë§ˆìŠ¤í¬ ì €ì¥
            #     # if save_frames:
            #     #     cv2.imwrite(str(frames_dir / f"frame_{frame_idx:06d}.jpg"), overlay)
            #     # if save_masks:
            #     #     cv2.imwrite(str(masks_dir / f"mask_{frame_idx:06d}.png"), mask_only)
            #     # ê°œë³„ í”„ë ˆì„ ì €ì¥
            #     if save_frames:
            #         if masks_only:
            #             cv2.imwrite(str(frames_dir / f"mask_{frame_idx:06d}.png"), final_output)
            #         else:
            #             cv2.imwrite(str(frames_dir / f"frame_{frame_idx:06d}.jpg"), final_output)
                
            #     pbar.update(1)
                
            #     # ë©”ëª¨ë¦¬ ê´€ë¦¬
            #     if frame_idx % 30 == 0:
            #         torch.cuda.empty_cache() if torch.cuda.is_available() else None
            #         gc.collect()
            
            # pbar.close()
            # ë‹¨ì¼ ìŠ¤ë ˆë“œ ì²˜ë¦¬ ë¶€ë¶„ì—ì„œë„ ì ìš©
            with torch.inference_mode():  # ì „ì²´ ì¶”ë¡  ë£¨í”„ë¥¼ ê°ìŒˆ
                pbar = tqdm(total=total_frames, desc="Processing")
                
                for frame_idx in range(total_frames):
                    ret, frame = cap.read()
                    if not ret:
                        break
                    
                    # ì¶”ë¡  - ëª¨ë“  ì²˜ë¦¬ê°€ for ë£¨í”„ ì•ˆì— ìˆì–´ì•¼ í•¨
                    start_time = time.time()
                    input_tensor = self.preprocess_frame(frame)
                    output = self.model(input_tensor)
                    
                    segmentation_mask = self.postprocess_output(output, frame.shape[:2])
                    inference_time = time.time() - start_time
                    inference_times.append(inference_time)
                    
                    # masks_onlyì— ë”°ë¼ ë‹¤ë¥¸ ì²˜ë¦¬
                    if masks_only:
                        final_output, class_info = self.create_opencv_bitwise_mask_visualization(segmentation_mask)
                    else:
                        final_output, class_info = self.create_enhanced_overlay(frame, segmentation_mask)
                    
                    # í†µê³„ ìˆ˜ì§‘
                    for class_name, count in class_info.items():
                        if class_name not in class_statistics:
                            class_statistics[class_name] = []
                        class_statistics[class_name].append(count)
                    
                    # ì„±ëŠ¥ ì •ë³´ í‘œì‹œ (ë§ˆìŠ¤í¬ ëª¨ë“œì¼ ë•ŒëŠ” í‘œì‹œí•˜ì§€ ì•ŠìŒ)
                    if show_stats and not masks_only:
                        fps_text = f"FPS: {1/inference_time:.1f}"
                        model_text = f"Model: {self.model_name}"
                        cv2.putText(final_output, fps_text, (10, 30), 
                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                        cv2.putText(final_output, model_text, (10, 60), 
                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                    
                    # ë¹„ë””ì˜¤ ì €ì¥
                    out.write(final_output)
                    
                    # ê°œë³„ í”„ë ˆì„ ì €ì¥
                    if save_frames:
                        if masks_only:
                            cv2.imwrite(str(frames_dir / f"mask_{frame_idx:06d}.png"), final_output)
                        else:
                            cv2.imwrite(str(frames_dir / f"frame_{frame_idx:06d}.jpg"), final_output)
                    
                    pbar.update(1)
                    
                    # ë©”ëª¨ë¦¬ ê´€ë¦¬
                    if frame_idx % 30 == 0:
                        torch.cuda.empty_cache() if torch.cuda.is_available() else None
                        gc.collect()
                
                pbar.close()
        
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        cap.release()
        out.release()
        # if mask_out is not None:
        #     mask_out.release()
        
        # ì„±ëŠ¥ í†µê³„ ì¶œë ¥
        # self.print_performance_stats(inference_times, class_statistics, total_frames, output_path)
        mode_text = "Masks Only" if masks_only else "Overlay"
        print(f"\n=== {mode_text} Processing Completed ===")
        self.print_performance_stats(inference_times, class_statistics, total_frames, output_path)
        
        # if save_masks:
        #     print(f"âœ“ Mask video saved to: {mask_output_path}")
        #     print(f"âœ“ Individual masks saved to: output_masks/")
        #     print(f"âœ“ Class legend saved to: output_masks/class_legend.png")
        if save_frames:
            if masks_only:
                print(f"âœ“ Individual masks saved to: output_masks/")
                print(f"âœ“ Class legend saved to: output_masks/class_legend.png")
            else:
                print(f"âœ“ Individual frames saved to: output_frames/")
    
    def print_performance_stats(self, inference_times, class_statistics, total_frames, output_path):
        """ì„±ëŠ¥ í†µê³„ ì¶œë ¥"""
        print(f"\n=== Performance Statistics ===")
        print(f"Model: {self.model_name} ({self.model_info['params']})")
        print(f"Total frames processed: {len(inference_times)}")
        
        if inference_times:
            avg_fps = 1 / np.mean(inference_times)
            min_fps = 1 / np.max(inference_times)
            max_fps = 1 / np.min(inference_times)
            
            print(f"Average FPS: {avg_fps:.2f}")
            print(f"Min FPS: {min_fps:.2f}")
            print(f"Max FPS: {max_fps:.2f}")
            print(f"Average inference time: {np.mean(inference_times)*1000:.1f}ms")
        
        print(f"\n=== Segmentation Statistics ===")
        for class_name, counts in class_statistics.items():
            if counts:  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš°ë§Œ
                avg_pixels = np.mean(counts)
                max_pixels = np.max(counts)
                percentage = (avg_pixels / (512 * 512)) * 100  # ì…ë ¥ í¬ê¸° ê¸°ì¤€
                # í•´ë‹¹ í´ë˜ìŠ¤ì˜ ìƒ‰ê¹” ì°¾ê¸°
                color_info = ""
                if class_name in self.class_names:
                    class_index = self.class_names.index(class_name)
                    if class_index < len(self.class_colors):
                        rgb = self.class_colors[class_index]
                        color_name = self.rgb_to_color_name(rgb)
                        color_info = f" ({color_name})"

                print(f"{class_name}: avg {avg_pixels:.0f} pixels ({percentage:.1f}%), max {max_pixels:.0f}{color_info}")
        
        print(f"\nOutput saved to: {output_path}")
        
        # ë©”ëª¨ë¦¬ ì •ë³´
        if torch.cuda.is_available():
            memory_allocated = torch.cuda.memory_allocated() / (1024**3)
            memory_cached = torch.cuda.memory_reserved() / (1024**3)
            print(f"\nGPU Memory - Allocated: {memory_allocated:.2f}GB, Cached: {memory_cached:.2f}GB")

    def rgb_to_color_name(self, rgb):
        """RGB ê°’ì„ ìƒ‰ê¹” ì´ë¦„ìœ¼ë¡œ ë³€í™˜"""
        r, g, b = rgb
        
        # Cityscapes ìƒ‰ìƒ ë§¤í•‘
        color_map = {
            (128, 64, 128): "purple",      # road/sidewalk
            (244, 35, 232): "pink",        # person  
            (70, 70, 70): "dark_gray",     # building
            (102, 102, 156): "light_purple", # wall
            (190, 153, 153): "light_brown", # fence
            (153, 153, 153): "gray",       # pole
            (250, 170, 30): "orange",      # traffic light
            (220, 220, 0): "yellow",       # traffic sign
            (107, 142, 35): "olive",       # vegetation
            (152, 251, 152): "light_green", # terrain
            (70, 130, 180): "sky_blue",    # sky
            (220, 20, 60): "crimson",      # person
            (255, 0, 0): "red",            # rider
            (0, 0, 142): "blue",           # car
            (0, 0, 70): "dark_blue",       # truck
            (0, 60, 100): "teal",          # bus
            (0, 80, 100): "dark_teal",     # train
            (0, 0, 230): "bright_blue",    # motorcycle
            (119, 11, 32): "dark_red",     # bicycle
        }
        
        rgb_tuple = (r, g, b)
        return color_map.get(rgb_tuple, f"rgb({r},{g},{b})")

    # # 1. RGB ê°’ì„ ìƒ‰ê¹” ì´ë¦„ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ ì¶”ê°€
    # def rgb_to_color_name(self, rgb):
    #     """RGB ê°’ì„ ìƒ‰ê¹” ì´ë¦„ìœ¼ë¡œ ë³€í™˜"""
    #     r, g, b = rgb
        
    #     # ìƒ‰ê¹” ë§¤í•‘ ë”•ì…”ë„ˆë¦¬
    #     color_map = {
    #         # ê¸°ë³¸ ìƒ‰ìƒë“¤
    #         (255, 0, 0): "red",
    #         (0, 255, 0): "green", 
    #         (0, 0, 255): "blue",
    #         (255, 255, 0): "yellow",
    #         (255, 0, 255): "magenta",
    #         (0, 255, 255): "cyan",
    #         (255, 255, 255): "white",
    #         (0, 0, 0): "black",
    #         (128, 128, 128): "gray",
    #         (255, 165, 0): "orange",
    #         (128, 0, 128): "purple",
    #         (255, 192, 203): "pink",
    #         (165, 42, 42): "brown",
    #         (0, 128, 0): "dark_green",
    #         (0, 0, 128): "navy",
    #         (128, 0, 0): "maroon",
            
    #         # Cityscapes íŠ¹ì • ìƒ‰ìƒë“¤
    #         (128, 64, 128): "purple",      # road/sidewalk
    #         (244, 35, 232): "pink",        # person  
    #         (70, 70, 70): "dark_gray",     # building
    #         (102, 102, 156): "light_purple", # wall
    #         (190, 153, 153): "light_brown", # fence
    #         (153, 153, 153): "gray",       # pole
    #         (250, 170, 30): "orange",      # traffic light
    #         (220, 220, 0): "yellow",       # traffic sign
    #         (107, 142, 35): "olive",       # vegetation
    #         (152, 251, 152): "light_green", # terrain
    #         (70, 130, 180): "sky_blue",    # sky
    #         (220, 20, 60): "crimson",      # person
    #         (0, 0, 142): "blue",           # car
    #         (0, 0, 70): "dark_blue",       # truck
    #         (0, 60, 100): "teal",          # bus
    #         (0, 80, 100): "dark_teal",     # train
    #         (0, 0, 230): "bright_blue",    # motorcycle
    #         (119, 11, 32): "dark_red",     # bicycle
    #     }
        
    #     # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ìƒ‰ìƒ ì°¾ê¸°
    #     rgb_tuple = (r, g, b)
    #     if rgb_tuple in color_map:
    #         return color_map[rgb_tuple]
        
    #     # ê°€ì¥ ê°€ê¹Œìš´ ìƒ‰ìƒ ì°¾ê¸°
    #     min_distance = float('inf')
    #     closest_color = "unknown"
        
    #     for color_rgb, color_name in color_map.items():
    #         # ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê³„ì‚°
    #         distance = sum((a - b) ** 2 for a, b in zip(rgb_tuple, color_rgb)) ** 0.5
    #         if distance < min_distance:
    #             min_distance = distance
    #             closest_color = color_name
        
    #     # ê±°ë¦¬ê°€ ë„ˆë¬´ ë©€ë©´ RGB ê°’ ê·¸ëŒ€ë¡œ í‘œì‹œ
    #     if min_distance > 100:
    #         return f"rgb({r},{g},{b})"
        
    #     return closest_color

    # (ì„ íƒì‚¬í•­) í´ë˜ìŠ¤ë³„ ìƒ‰ìƒ ë²”ë¡€ë„ ê°œì„  
    def save_enhanced_mask_legend(self, save_dir):
        """ìƒ‰ê¹” ì´ë¦„ì´ í¬í•¨ëœ í´ë˜ìŠ¤ë³„ ìƒ‰ìƒ ë²”ë¡€ ì €ì¥"""
        legend_height = len(self.class_names) * 35 + 50  # ì¡°ê¸ˆ ë” ë†’ê²Œ
        legend_width = 500  # ì¡°ê¸ˆ ë” ë„“ê²Œ
        legend = np.ones((legend_height, legend_width, 3), dtype=np.uint8) * 255
        
        # ì œëª©
        cv2.putText(legend, "Segmentation Classes", (10, 30), 
                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)
        
        # ê° í´ë˜ìŠ¤ë³„ ìƒ‰ìƒê³¼ ì´ë¦„
        for i, (class_name, color) in enumerate(zip(self.class_names, self.class_colors)):
            y_pos = 60 + i * 30
            
            # ìƒ‰ìƒ ë°•ìŠ¤
            cv2.rectangle(legend, (10, y_pos - 12), (40, y_pos + 12), 
                        color.tolist(), -1)
            cv2.rectangle(legend, (10, y_pos - 12), (40, y_pos + 12), 
                        (0, 0, 0), 1)
            
            # ìƒ‰ê¹” ì´ë¦„ ì¶”ê°€
            color_name = self.rgb_to_color_name(color)
            
            # í´ë˜ìŠ¤ ì´ë¦„ + ìƒ‰ê¹” ì´ë¦„
            text = f"{i}: {class_name} ({color_name})"
            cv2.putText(legend, text, (50, y_pos + 5), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 0), 1)
        
        # ë²”ë¡€ ì €ì¥
        legend_path = save_dir / "enhanced_class_legend.png"
        cv2.imwrite(str(legend_path), legend)
        print(f"âœ“ Enhanced class legend saved to: {legend_path}")

    # (ì„ íƒì‚¬í•­) í„°ë¯¸ë„ì—ì„œ ì‹¤ì‹œê°„ ìƒ‰ê¹” ì •ë³´ í‘œì‹œ 
    def print_color_mapping(self):
        """í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ìƒ‰ê¹” ë§¤í•‘ ì¶œë ¥"""
        print(f"\n=== Color Mapping for {len(self.class_names)} Classes ===")
        for i, (class_name, color) in enumerate(zip(self.class_names, self.class_colors)):
            color_name = self.rgb_to_color_name(color)
            rgb_str = f"RGB({color[0]}, {color[1]}, {color[2]})"
            print(f"{i:2d}: {class_name:15s} -> {color_name:12s} {rgb_str}")
        print()

    # def create_minimal_mask_visualization(self, segmentation_mask):
    #     """ìµœì†Œí•œì˜ ì—°ì‚°ìœ¼ë¡œ ë§ˆìŠ¤í¬ ìƒì„±"""
        
    #     # ğŸš€ ê°€ì¥ ë‹¨ìˆœí•œ ë°©ì‹ (ê¸°ì¡´ê³¼ ê±°ì˜ ë™ì¼í•˜ì§€ë§Œ ìµœì í™”)
    #     colored_mask = self.class_colors[segmentation_mask % len(self.class_colors)]
    #     colored_mask = colored_mask[..., ::-1]  # RGB -> BGR
        
    #     # í†µê³„ ìƒëµ
    #     return colored_mask, {}
    def create_opencv_bitwise_mask_visualization(self, segmentation_mask):
        """OpenCV bitwise ì—°ì‚° í™œìš© - ë§¤ìš° ë¹ ë¦„!"""
        
        h, w = segmentation_mask.shape
        result = np.zeros((h, w, 3), dtype=np.uint8)
        
        for class_id in range(len(self.class_colors)):
            # í´ë˜ìŠ¤ ë§ˆìŠ¤í¬ ìƒì„±
            class_mask = (segmentation_mask == class_id).astype(np.uint8) * 255
            
            if np.any(class_mask):
                # ìƒ‰ìƒ ì´ë¯¸ì§€ ìƒì„±
                color_bgr = self.class_colors[class_id][::-1]
                color_img = np.full((h, w, 3), color_bgr, dtype=np.uint8)
                
                # ğŸš€ OpenCV bitwise_and ì‚¬ìš© (í•˜ë“œì›¨ì–´ ìµœì í™”!)
                masked_color = cv2.bitwise_and(color_img, color_img, mask=class_mask)
                
                # ğŸš€ OpenCV bitwise_orë¡œ í•©ì„±
                result = cv2.bitwise_or(result, masked_color)
        
        # ğŸ”¥ í†µê³„ ì™„ì „ ìƒëµ
        return result, {}

def main():
    parser = argparse.ArgumentParser(description="Optimized EfficientViT Segmentation for Jetson")
    parser.add_argument("--input", "-i", required=True, help="Input video path")
    parser.add_argument("--output", "-o", default="output_segmented.mp4", help="Output video path")
    parser.add_argument("--model", "-m", default="efficientvit_seg_b0", 
                       choices=list(EfficientViTModelManager.AVAILABLE_MODELS.keys()),
                       help="EfficientViT model name")
    parser.add_argument("--device", "-d", default="cuda", help="Device (cuda/cpu)")
    # parser.add_argument("--save-frames", action="store_true", help="Save individual overlay frames")
    parser.add_argument("--save-frames", action="store_true", help="Save individual frames")
    # parser.add_argument("--save-masks", action="store_true", help="Save individual mask frames and mask video")
    parser.add_argument("--save-masks", action="store_true", help="Output masks only (instead of overlay)")
    parser.add_argument("--no-optimize", action="store_true", help="Skip Jetson optimization")
    parser.add_argument("--single-thread", action="store_true", help="Use single thread processing")
    parser.add_argument("--list-models", action="store_true", help="List available models")
    parser.add_argument("--no-stats", action="store_true", help="Hide performance overlay")
    parser.add_argument("--class-mapping", default="auto", 
                       choices=["auto", "cityscapes", "ade20k", "pascal_voc", "custom_walkway"],
                       help="Class mapping dataset (auto: detect from model output)")
    parser.add_argument("--show-classes", action="store_true", help="Show detected classes and exit")
    
    args = parser.parse_args()
    
    # ëª¨ë¸ ëª©ë¡ ì¶œë ¥
    if args.list_models:
        EfficientViTModelManager.list_models()
        return
    
    # ì…ë ¥ íŒŒì¼ í™•ì¸
    if not Path(args.input).exists():
        print(f"Error: Input file {args.input} not found!")
        return
    
    try:
        # ì¶”ë¡  ê°ì²´ ìƒì„±
        inferencer = OptimizedEfficientViTInference(
            model_name=args.model, 
            device=args.device,
            optimize_jetson=not args.no_optimize,
            class_mapping=args.class_mapping
        )
        
        # í´ë˜ìŠ¤ ì •ë³´ë§Œ ì¶œë ¥í•˜ê³  ì¢…ë£Œ
        if args.show_classes:
            print(f"\n=== Detected Classes ({len(inferencer.class_names)}) ===")
            for i, class_name in enumerate(inferencer.class_names):
                color = inferencer.class_colors[i]
                color_name = inferencer.rgb_to_color_name(color)
                print(f"{i:3d}: {class_name:20s} RGB({color[0]:3d}, {color[1]:3d}, {color[2]:3d}) ({color_name})")
            return
        
        # ë¹„ë””ì˜¤ ì²˜ë¦¬
        inferencer.process_video_optimized(
            args.input, 
            args.output, 
            save_frames=args.save_frames,
            save_masks=args.save_masks,
            show_stats=not args.no_stats,
            multithreading=not args.single_thread
        )
        
    except Exception as e:
        print(f"Error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
